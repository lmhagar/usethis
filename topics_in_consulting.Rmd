---
title: "Topics in Statistical Consulting"
author: |
  | **Statistical Consulting and Collaborative Research Unit**
  | **University of Waterloo**
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
biblio-style: apalike
link-citations: true
bibliography: references.bib
output: 
  bookdown::gitbook:
    fig_caption: yes
    includes:
      in_header: preamble.html
---

```{r setup, include=FALSE}
# common functions
# contributors should be aware of these and use them whenever possible.
source("common_functions.R")
```

# Preface {-}

This book aims to give introductions to some topics in statistical consulting. We hope that this will help audiences who have light background in statistics.

For contributors to this book, please refer to the Appendix for [General Guidelines], [Formatting Guidelines] and [Github tutorials][Git and GitHub: A Quick Tutorial].

For questions regarding the materials of this book, please [contact us](https://uwaterloo.ca/statistical-consulting-and-collaborative-research-unit/).

## Authors  {-}

- Trang Bui
- Meixi Chen
- Luke Hagar
- Kelly Ramsay
- Yuliang Shi
- Grace Tompkins
- Feiyu Zhu

## Editors {-}

- Martin Lysy
- Glen McGee
- Joslin Goh


<!--chapter:end:index.Rmd-->

# Introduction to R 

*Author: Joslin Goh, Trang Bui*

*Last Updated: Feb 04, 2021*

--- 

## R and RStudio

[R](https://www.r-project.org/) is a software environment for statistical computing and graphics. Unlike other statistical software, R is free. Besides built-in functions, additional packages for solving many different statistical or application problems are made and maintained by contributors around the world. This makes R an attractive and popular statistical tool nowadays.

[RStudio](https://rstudio.com/products/rstudio/) is an integrated development environment ([IDE](https://en.wikipedia.org/wiki/Integrated_development_environment)) for R. It is easier to work with R using RStudio. 

```{r rintro-rstudio, fig.cap="The RStudio interface", echo=FALSE, fig.align='center'}
knitr::include_graphics("data/images/rintro-rstudio.png")
```

The interface of RStudio shown in Figure \@ref(fig:rintro-rstudio) contains four panes:

- [Source Editor], 
- Console, 
- Workspace Browser, and 
- Files (and Plots, Packages, Help, and Viewer).

The four panes can be positioned differently based on personal preference. Figure \@ref(fig:rintro-rstudio) shows the default position. In this section, we will mainly be using the Source Editor and Console panes. Readers are encouraged to refer to other [resources](https://robwschlegel.github.io/Intro_R_Workshop/rstudio.html#the-panes-of-rstudio) on the use of other panes.

## Basic R

### Calculating with R {#rintro-calculate}

In its simplest form, R can be used as a calculator. In the R Console area, type:
```{r rintro-basic-sum, echo = TRUE, eval = FALSE}
1 + 2
```
The following will be printed in the R Console area:
```{r rintro-basic-sum-result, echo = FALSE, eval = TRUE, comment = NA}
1 + 2
```

Subtraction can be done in a similar way:
```{r rintro-basic-subtract, echo = TRUE, eval = TRUE, comment = NA}
5 - 10
```

Other basic operations such as multiplication, division, and powers are also included.
```{r rintro-basic-operations, echo = TRUE, comment = NA}
9 * 26

100 / 7.5

2^3
```

Some basic operations involve built-in functions in R. For example,

- Square root:
    ```{r rintro-basic-squareroot, echo = TRUE, eval = TRUE, comment = NA}
sqrt(25)
    ```
- Logarithm:
    ```{r rintro-basic-log10, echo = TRUE, eval = TRUE, comment = NA}
log(10, base = 10)
    ```
- Natural logarithm:
    ```{r rintro-basic-log, echo = TRUE, eval = TRUE, comment = NA}
log(10)
    ```

### Variables

Variables are useful when they need to be used repeatedly or to be recalled in the future. 

For example, suppose we are interested in evaluating
$$
\frac{e^{1-9.2315468}}{1-e^{1-9.2315468}},
$$
we can store the repeated value $9.2315468$ as a variable before performing the calculation. 

To store the value as the variable $x$, we can type
```{r rintro-basic-variable-assign, echo = TRUE, eval = TRUE, comment = NA}
x <- 9.2315468
```

Note that:

- In the Console pane, nothing is returned.
- In the Environment tab under the Workspace Browser pane, $x$ appears together with the value it represents. This shows that the current workspace recognizes $x$ as $9.2315468$.
- Now if we try typing $x$ in the Console, we will see the value it represents.
    ```{r rintro-basic-variable-result, echo = TRUE, eval = TRUE, comment = NA}
x
    ```

Back to our example, we wanted to evaluate
$$
\frac{e^{1-9.2315468}}{1-e^{1-9.2315468}},
$$
Since $x = 9.2315468$ is in our work environment, we can now type
```{r rintro-basic-variable-calculate, echo = TRUE, eval = TRUE, comment = NA}
exp(1 - x) / (1 - exp(1 - x))
```

In R, there are built-in variables, which are called default variables in R. The number $\pi$ is recognized as `pi`. Another default variable is the imaginary number, i.e $\sqrt{-1}$, which is recorded as `i` in R. 

### Vectors

Oftentimes, we encounter sequences of numbers during data analysis. For example, 
the height of 10 students, the grades of the ECON 101 students in the Fall term, the age of the attendees, etc.

In R, sequences of numbers can be recorded as vectors. 

Suppose there are five people in a class. The ages of the people in the class are:
$$
18, 21, 19, 20, 21
$$
We can create a vector for our record as below.
```{r rintro-basic-vector, echo = TRUE, eval = FALSE, comment = NA}
age <- c(18, 21, 19, 20, 21)
```
In the Workspace Browser pane, we can see the variable `age` with the values that we have given. And if we type `age` in the Console pane, we get these values printed in the Console.

Vectors may not appear to be useful for many since most of the popular functions are ready for use. But for those intending to create their own R functions, it is important to understand how to create and manipulate vectors. Many [comparators and logical operators](https://www.statmethods.net/management/operators.html) such as those discussed in Section [1.2.1](#rintro-calculate) work on both vectors and scalars. These calculations will be element-wise. 

## Basic Data Analysis Workflow

### Reading Data into R

#### Setting Working Directory

To start, it is important to inform R the directory that the data file is stored. For Mac/Windows users of RStudio, choose <kbd>Session</kbd> > <kbd>Set Working Directory</kbd> > <kbd>Choose Directory</kbd>. 

The function `setwd()` can also be used to set the working directory if the directory string is available. For example,
```{r rintro-data-setwd, echo = TRUE, eval = FALSE, comment = NA}
setwd("D:/")
```
will set the working directory to "D:/".

#### Importing the Data

In the real world, data are recorded in different formats such as Excel spreadsheet (`xls`), Comma Separated Values (`csv`) or Text (`txt`). Each row of a data file is an observation while each column is a variable or a feature. 

Data are imported into the R Environment using functions such as `read.csv()` and `read.table()`. Imported data are stored as a data frame object. In this section, we will look at two data sets: [`caliRain.csv`](data/caliRain.csv) and [`drinks.csv`](data/drinks.csv).

Suppose we saved the data sets in a subfolder called `data` in the working directory. We can import both data sets `caliRain.csv` and `drinks.csv` into the R environment and save them as data frames called `drinks_df` and `rain_df` respectively.
```{r rintro-data-import, echo=TRUE, eval=TRUE, comment = NA}
drinks_df <- read.csv("data/drinks.csv")
rain_df <- read.csv("data/caliRain.csv")
```

#### A Look at the Data 

It is important to take a look at the data set imported into the environment before performing the analysis. To view `rain_df` as a table,

```{r rintro-call-common-function, echo = FALSE, eval = TRUE, comment = NA}
source("common_functions.R")
```

```{r rintro-data-view, echo=TRUE, eval=FALSE, comment = NA}
View(rain_df)
```
The function `head()` can also show the first few rows of the data set.  
```{r rintro-data-head, echo=TRUE, eval=TRUE, comment = NA}
head(rain_df)
```

The `caliRain.csv` file contains daily rainfall recorded at numerous meteorological stations monitored by the state of California. The variables recorded are:
  
- `STATION`: Name of the station,
- `PRECIP`: precipitation (inches),
- `ALTITUDE`: altitude (feet),
- `LATITUDE`: latitude (feet),
- `DISTANCE`: distance to the Pacific Ocean (miles), and
- `SHADOW`: slope face (1: Westward, 2:Leeward).

The variables `STATION` and `SHADOW` are categorical variables, whereas the remaining are continuous variables. 

#### Accessing the Data Frame 

Oftentimes, we are interested in accessing an individual column (or variable) within the data frame. For example, if we are interested in the `PRECIP` variable in the data set `caliRain.csv` (which is now stored as `rain_df`). There are two ways to access the column:

- Use the dollar sign followed by the name of the variable.
    ```{r rintro-data-dollar, echo=TRUE, eval=TRUE, comment = NA}
rain_df$PRECIP
    ```
- Use the number of the column in the data set. 
    ```{r rintro-data-col, echo=TRUE, eval=TRUE, comment = NA}
rain_df[, 2]
    ```

Similarly, there are times we want to investigate a particular row (or observation). Suppose we are interested in the 10th observation, type
```{r rintro-data-row, echo=TRUE, eval=TRUE, comment = NA}
rain_df[10, ]
```

We can also access a specific cell in the data. If we want to access the precipitation of the 5th observation, we can do either one of the following: 
```{r rintro-data-cell, echo=TRUE, eval=FALSE, comment = NA}
rain_df$PRECIP[5]
rain_df[5, 2]
```

Accessing a random variable, an observation or a specific value coming from an observation are all useful for data management and manipulation purpose. 

#### Modifying the Data Frame

Sometimes, we want to make changes to the data frame such as

- making changes to existing records,
- adding new observations or variables, or
- removing outliers from the data set.

If we want to change the existing records, we need to identify which records we are interested to change.

- a variable, i.e. a column, or 
- a specific observation.

##### Modifying a Variable

To modify a variable, we need to 

- identify the name or the column of the variable to access it in the data frame,
- decide on the modification or conversion, and 
- decide on how to store the new variable. 

We recommend storing the conversion as a new variable in the data frame to avoid confusion.

Suppose we are interested to analyze `DISTANCE` in meters ($1 \tx{ ft} = 0.3048 \tx{ m}$). We can make the conversion and save it as a new column called `DISTANCE_M` in the data set. 
```{r rintro-data-modcol, echo=TRUE, eval=TRUE,comment=NA}
rain_df$DISTANCE_M <- rain_df$DISTANCE * 0.3048
```

##### Modifying a Specific Observation

To modify a specific observation, we need to

- identify how to access the variable in the data frame,
- decide on the modification, and 
- decide on how to store the new variable. 

Suppose the distance for Eureka station is entered incorrectly and is supposed to be 1.5 feet instead. To replace this value, type
```{r rintro-data-modcell, echo=TRUE, eval=TRUE,comment=NA}
rain_df$DISTANCE[1] <- 1.5
```

##### Removing Records

To remove an entire column from a data frame, 
```{r rintro-data-rmcol, echo=TRUE, eval=FALSE,comment=NA}
rain_df <- rain_df[, -COLUMN_NUMBER]
```

To remove an entire row from a data frame, 
```{r rintro-data-rmrow, echo=TRUE, eval=FALSE,comment=NA}
rain_df <- rain_df[-ROW_NUMBER, ]
```

This way we re-store `rain_df` with the new data frame `rain_df` where its row/column has been removed.

#### Data Structure

The structure that R stores the data can be viewed using the function `str()`.   
```{r rintro-data-str, echo=TRUE, eval=TRUE, comment = NA}
str(rain_df)
```
Here, the variable `SHADOW` is recorded as a numeric value. This is not an accurate depiction of the data set. 

To ensure the analysis can be done properly, we need to convert the values in `SHADOW` into categorical values in the data set. To do so, we use the function `factor()`.
```{r rintro-data-factor, echo=TRUE, eval=TRUE, comment = NA}
rain_df$SHADOW <- factor(rain_df$SHADOW,
  levels = c("1", "2"),
  labels = c("Westward", "Leeward")
)
```
Here, the numerical values 1 and 2 are set to “Westward” and “Leeward”, respectively. 

Now, when we check the structure of the data set after the transformation, the variable `SHADOW` is now stored as a categorical variable (or factor).
```{r rintro-data-factor-str, echo=TRUE, eval=TRUE, comment = NA}
str(rain_df)
```

### Descriptive Statistics

We will use the `PRECIP` variable to demonstrate how common statistics are computed.

- [Mean](https://en.wikipedia.org/wiki/Mean) or average of a sequence of numbers can be obtained using the function `mean()`.
    ```{r rintro-data-mean, echo=TRUE, eval=TRUE, comment=NA}
mean(rain_df$PRECIP)
    ```
- [Median](https://en.wikipedia.org/wiki/Median) of a sequence of numbers can be obtained using the function `median()`.
    ```{r rintro-data-median, echo=TRUE, eval=TRUE, comment=NA}
median(rain_df$PRECIP)
    ```
- [Variance and standard deviation](https://en.wikipedia.org/wiki/Variance) of a sequence of numbers can be obtained using the functions `var()` and `sd()` respectively.
    ```{r rintro-data-var, echo=TRUE, eval=TRUE, comment=NA}
var(rain_df$PRECIP)
sd(rain_df$PRECIP)
    ```
- The minimum and maximum of a set of numbers can be obtained through functions `min()` and `max()`. 
    ```{r rintro-data-minmax, echo=TRUE, eval=TRUE, comment=NA}
min(rain_df$PRECIP)
max(rain_df$PRECIP)
    ```
    The function `range()` also shows the minimum and maximum values.
    ```{r rintro-data-range, echo=TRUE, eval=TRUE, comment=NA}
range(rain_df$PRECIP)
    ```
    
### Data Visualization

There is a wide variety of plots that can be created using R, but we will focus on some of our favorites:

- bar graphs: show the distributions of categorical variables,
- boxplots: show the five-number summaries of continuous variables, and
- histograms: show the distributions of continuous variables.

#### Categorical Variables

In order to create bar graphs, we need to summarize data using tables.

The numerical summary of a categorical variable are usually summarized in a table:
```{r rintro-data-table-shadow, echo=TRUE, eval=TRUE, comment=NA}
count_of_shadow <- table(rain_df$SHADOW)
count_of_shadow
```

A cross-tabulation table (or [contingency table](https://en.wikipedia.org/wiki/Contingency_table)) can also be done. Suppose we are interested to create cross-tab for the variables `hasMilk` and `temp` in the `drinks_df`, we can do the following:
```{r rintro-data-table-milk, echo=TRUE, eval=TRUE, comment=NA}
table_of_milk_by_temp <- table(
  drinks_df$hasMilk,
  drinks_df$temp
)
table_of_milk_by_temp
```

Sometimes it is more useful to report the proportions, which can be converted into percentages. To do so, we use the function `prop.table()`.
```{r rintro-data-table-shadow-prop, echo=TRUE, eval=TRUE, comment=NA}
prop.table(count_of_shadow)
```

For a contingency table, the default `prop.table()` function will output the proportions based on the entire data set.
```{r rintro-data-table-milk-prop, echo=TRUE, eval=TRUE, comment=NA}
prop.table(table_of_milk_by_temp)
```

Suppose we are interested in the percentages of the hot drinks that contain milk, we will want to report the proportion by column (`Temperature`).
```{r rintro-data-table-milk-prop-col, echo=TRUE, eval=TRUE, comment=NA}
prop.table(table_of_milk_by_temp, 2)
```
These values are the proportions of drinks which contains milk (or not) conditioning on whether the drink is cold or hot, i.e., the values are normalized by the columns. 

#### Bar Graphs

[Bar graphs](https://www.statmethods.net/graphs/bar.html) are commonly used to visualize categorical variables. We can make a bar graph from the count table using the function `barplot()` in R.

```{r rintro-data-barplot, echo=TRUE, eval=TRUE, fig.height=4.5}
barplot(count_of_shadow,
  main = "Distribution of shadow",
  xlab = "Shadow", ylab = "Frequency"
)
```

#### Boxplots

The [boxplot](https://en.wikipedia.org/wiki/Box_plot) is a visual representation of the five-number summary that can give us a sense of the distribution of the variable.

- minimum,
- first quartile, $Q_1$,
- second quartile, i.e., median,
- third quartile, $Q_3$, and
- maximum.

Potential outliers are shown as dots outside the boxplots.

The boxplot of `PRECIP` shows some potential outliers.

```{r rintro-data-boxplot, echo=TRUE, eval=TRUE, fig.height=5}
boxplot(rain_df$PRECIP,
  main = "Precipitation",
  ylab = "Inches"
)
```

Side-by-side boxplots are commonly used to visualize the relationship between a continuous variable and a categorical variable. The following is the boxplot of the precipitation by shadow. 

```{r rintro-data-boxplot-side, echo=TRUE, eval=TRUE, fig.height=5}
boxplot(rain_df$PRECIP ~ rain_df$SHADOW,
  main = "Precipitation",
  ylab = "Inches"
)
```

In the side-by-side boxplots, notice that there are no potential outliers. Compared to the whole data, certain observations can be considered as outliers. But if we group the data by `SHADOW`, the data are not outliers in their groups. 

#### Histograms

[Histograms](https://en.wikipedia.org/wiki/Histogram) are commonly used to visualize the distribution of continuous variables. When looking at histograms, pay attention to

- the shape: symmetric vs asymmetric,
- the center, and
- the spread.

To plot precipitation in a histogram,
```{r rintro-data-histogram, echo=TRUE, eval=TRUE, fig.height=5}
hist(rain_df$PRECIP,
  main = "Distribution of precipitation",
  xlab = "Precipitation", ylab = "Inches"
)
```

Notice that there is no space in between the bars like in the bar graph. This is because the graph is for continuous variables instead of categorical variables.

#### Scatterplots

Scatterplots are used to visualize the relationship between two continuous variables.

```{r rintro-data-scatter, echo=TRUE, eval=TRUE, fig.height=6}
plot(rain_df$DISTANCE, rain_df$PRECIP,
  main = "Relationship: precipitation vs distance",
  xlab = "Distance (ft)", ylab = "Precipitation (inches)"
)
```
<!-- The scatterplot shows some precipitation records that are exceptionally high when the distance is at 0ft and 150ft.  -->

#### A Fancy Visualization Library

The `r cran_link("ggplot2")` library is a package created by Hadley Wickham. It offers a powerful language to create elegant graphs. A basic introduction of this package can be found in a later [section][Introduction to ggplot2].

## Some Coding Tips

### Source Editor

It will be hard to remember and troublesome to re-write all the codes created in the Console every time, especially if there are many lines of code. The Source Editor allows us to write and save all codes into R code files. The lines of codes in the Source Editor are not processed by R unless executed by the user.

There are many ways the codes in the Code Editor can be executed: 

- Select the codes to process, click <kbd>Run</kbd> on the top right corner of the Source Editor.
- For Windows users, run the selected codes by pressing <kbd>Ctrl</kbd> + <kbd>Enter</kbd>. For Mac users, use <kbd>Command</kbd> + <kbd>Enter</kbd>.
- If we only want to run one line of code, place the cursor at the line of code, and use either one of the two ways mentioned above.

We recommend typing the codes in the Source Editor and then executing the codes. This way, there is a copy of what was done for future references.

### Commenting

Comment the codes! To do so, use \#.  R does not process anything behind \#. For example, 
```{r rintro-basic-comment, echo = TRUE, eval = FALSE, comment = NA}
# I am trying to like R!!!!
```

Everyone uses comments differently, but generally, comments are useful for understanding what is the code for and sometimes, the expected output.

To comment off a block of code, select the lines, and press <kbd>Ctrl</kbd> + <kbd>Shift</kbd> + <kbd>C</kbd>. Doing this a second time, the code section will be uncommented. 

In RStudio, if the following is done in the Code Editor:
```{r rintro-basic-comment-section, echo = TRUE, eval = FALSE, comment = NA}
# ----------------
# Try Me!
# ----------------
```
a triangle button will appear next to the line numbers at the beginning and end of the code section. Clicking the button will hide or unhide the section.

### Saving the Environment

When quitting R or RStudio, we can choose to save the Environment and History that we were working with in the files called `.RData` and `.RHistory` respectively. When we open the R code file next time, the two files will be automatically loaded. 
However, it is recommended not to save the Environment in the default way. Instead, start in a clean environment so that older objects do not remain in the environment any longer than they need to. If that happens, it can lead to unexpected results.

For those who want to save the Environment for future use, we recommend saving the Environment using the function `save.image()` rather than using the default files `.RData`. If we only want to save certain values, we can use the function `save()` and then load the saved Environment later using the `load()` function. 

### Installing and Loading Libraries

The R user community creates functions and data sets to share. They are called packages or libraries. The packages are free and can be installed as long as there is access to the Internet.

To install a library, say `r cran_link("ggplot2")`, you can either use the RStudio interface, or you can do it from the command line as follows:
```{r rintro-basic-install, echo=TRUE, eval=FALSE, comment=NA}
install.packages("ggplot2", dependencies=TRUE)
```
You only need to do this once in a while, e.g., when you install a new version of R.  Then, to use the package, include the following code at the beginning of the file:
```{r rintro-basic-require, echo=TRUE, eval=TRUE, comment=NA}
require(ggplot2)
```
This command needs to be run every time you want to use the package in a new R session.

### Good Coding Practices

- Start each program with a description of what it does.
- Load all required packages at the beginning.
- Consider the choice of working directory.
- Use comments to mark off sections of code.
- Put function definitions at the top of the file, or in a separate file if there are many.
- Name and style code consistently.
- Break code into small, discrete pieces.
- Factor out common operations rather than repeating them.
- Keep all of the source files for a project in one directory and use relative paths to access them.
- Have someone else review the code.
- Use version control.

## Getting Help

Before asking others for help, it is generally a good idea for you to try to help yourself.

### R Documentation

R has extensive documentation and resources for help. To read the documentation of a function, add a question mark before the name of a function. 

For example, to find out how to use the function `round()`, try
```{r rintro-basic-help, echo = TRUE, eval = FALSE, comment = NA}
?round
```
The description of the function and examples of how to use it will appear in the Files pane. In this example, as shown in the documentation, the function `round()` rounds the values in its first argument to the specified number of decimal places.

### Online Resources

There are a lot of basic functions or default variables that have not been mentioned so far. When analyzing data, we often encounter situations in which we need to use unknown or unfamiliar functions. In this case, we often rely on online search engines to find those functions. It is common practice to use online resources in real-world data analysis. Hence, readers are encouraged to explore the online resources.

<!--chapter:end:mod_intro_r.Rmd-->

# Introduction to ggplot2

*Author: Joslin Goh*

*Last Updated: Feb 09, 2021*

--- 

## Introduction

In this chapter, we assume that the readers have a basic understanding of R and RStudio. We have prepared a [chapter][Introduction to R] for those who need a quick introduction to R and RStudio.

`r cran_link("ggplot2")` is a data visualization package for R and RStudio. It is implemented based on @wilkinson12. The package can be installed and loaded using the command:
```{r visgg-load, echo = TRUE, eval = TRUE}
require("ggplot2")
```

The layers of a graph are shown in Figure \@ref(fig:visgg-grammar). In this chapter, we will show you how to build a plot layer by layer. 
```{r visgg-grammar, fig.cap="The layers of a graph", echo=FALSE, fig.align='center'}
knitr::include_graphics("data/images/visgg-grammarOfGraphics.png")
```

### Example Data Set

The examples shown in this chapter come from the data set `diamond` from the `r cran_link("ggplot2")` package.
```{r visgg-data-load, echo = TRUE, eval = TRUE}
data("diamonds")
```

The `diamond` data set consists of the price, quality information, and physical measurements of different diamonds. The structure of the data set is displayed using the function `str()`.
```{r visgg-data-str, echo = TRUE, eval = TRUE, comment = NA}
str(diamonds)
```

The `diamond` data set consists of many data points. To simplify the illustration, we will only use a subset of the data. To sample a subset:
```{r visgg-data-sample, echo = TRUE, eval = TRUE, comment = NA}
set.seed(2019)
my.diamonds <- diamonds[sample(nrow(diamonds), 100), ]
```
The function `set.seed()` ensures that the sample is consistent and replicable. 

## Data

The first step to graphing is to specify the data set and decide what goes on the axes. 

Suppose we want to investigate how the price of a diamond behaves with respect to its carat. Then, the two variables (or columns) involved are `price` and `carat`. The x-axis is usually the explanatory variable and the y-axis is the dependent variable. In this scenario, `price` should be on the y-axis and `carat` on the x-axis. 

To initiate this graph in `r cran_link("ggplot2")`, 
```{r visgg-plot-blank, echo = TRUE, eval = TRUE, fig.cap="A blank canvas.", comment = NA}
ggplot(my.diamonds, aes(x = carat, y = price))
```

The command creates a blank plot with no points or lines in it. The function does not assume the type of graphs it needs to produce unless it was told. Since this is the first (base) layer which will be used over and over again, it is best to save it as an object:
```{r visgg-plot-blank-save, echo = TRUE, eval = TRUE, comment = NA}
p <- ggplot(my.diamonds, aes(x = carat, y = price))
```

## Aesthetics

The first layer to be added onto the blank plot is a layer of the data points. In our case, we are interested to make a scatterplot that involves points that represent the data on the graph. The function `geom_point()` adds the necessary points onto the base layer.
```{r visgg-aes-point, echo = TRUE, eval = TRUE, fig.cap="A scatterplot of the price of diamond vs diamond carat.", comment = NA}
p + geom_point()
```

Each layer has its own components. For this layer, the common components include:

- `col`: the colour of the points specified using names, rgb specification or NA for transparent colour, 
- `size`: the size of the points specified in millimeters, and
- `shape`: the shape of the points.
        
### The Colour Component

A common way to specify the colour of the points is through the name of the colours. For example, `red`, `darkblue`, `magenta`, `chocolate` etc. A complete list of colours can be found [here](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf).

Suppose we want the points to appear blue, we can change it by using the option `col`.
```{r visgg-aes-point-blue, echo = TRUE, eval = TRUE, fig.cap="The colour of the points is set to blue.", comment = NA}
p + geom_point(col = "blue")
```

When `col=NA`, the points will become transparent:
```{r visgg-plot-point-NA, echo = TRUE, eval = TRUE, fig.cap="The points on the scatterplot has become invisible.", comment = NA, warning=FALSE}
p + geom_point(col = NA)
```

#### Setting vs Mapping

So far, we **set** the colour of the points to a specific colour of our choice. In some cases, we prefer the colour to change based on the information from another column (usually categorical) in the data set.

For example, suppose we want the colour of the points on the graph to change based on `cut`, which has 5 categories: `Fair`, `Good`, `Very Good`, `Premium` and `Ideal`. 
```{r visgg-aes-col-cut, echo = TRUE, eval = TRUE, fig.cap="Colouring the points based on the cut variable.", comment = NA}
p + geom_point(aes(col = cut))
```

This is called **mapping**.

#### Changing the Colour Palette

The choice of colours used in `aes()` is determined by the choice of the colour palette. When the choice is not mentioned, the default option is used. There are many online packages with pre-set palettes that you can use. We will show you the most common one known as `RColorBrewer`, which includes three types of colour palettes: sequential, diverging and qualitative.
```{r visgg-colbrew-display, echo=TRUE, eval=TRUE, fig.cap="Palettes available in RColorBrewer. The first chunk  shows palettes suitable for sequential categories, the middle chunk consists of palettes suitable for nominal categories whereas the last chunk of palettes are recommended for diverging categories.", comment=NA, fig.dim=c(8,10)}
require(RColorBrewer)
display.brewer.all()
```

Suppose we want to use the `BuGn` colour palette from `RColorBrewer` on the scatterplot created earlier, we can use the function `scale_colour_brewer()`:
```{r visgg-colbrew-set1-plot, echo = TRUE, eval = TRUE, fig.cap="The points are coloured with the BuGn colour palette which was recommended for sequential categories.", comment = NA}
p1 <- p + geom_point(aes(col = cut))
p1 + scale_colour_brewer(palette = "BuGn")
```

Readers can refer [here](https://www.datanovia.com/en/blog/the-a-z-of-rcolorbrewer-palette/) for more information about `RColorBrewer`.

Our preference is to use a colour-blind friendly palette such as:
```{r visgg-colblind-grey, fig.cap="Colour blind friendly palette (grey)", echo=FALSE, fig.align='center'}
knitr::include_graphics("data/images/visgg-cbPalette-grey.png")
```

```{r visgg-colblind-black, fig.cap="Colour blind friendly palette (black)", echo=FALSE, fig.align='center'}
knitr::include_graphics("data/images/visgg-cbPalette.png")
```

Both palettes are not part of `RColorBrewer` and are extracted from [Cookbook for R](http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/). They are coded as follows: 
```{r visgg-colblind-extract, echo = TRUE, eval = TRUE, comment = NA}
# colour blind friendly palette with grey
cbgPalette <- c(
  "#999999", "#E69F00", "#56B4E9", "#009E73",
  "#F0E442", "#0072B2", "#D55E00", "#CC79A7"
)
# colour blind friendly palette with black
cbbPalette <- c(
  "#000000", "#E69F00", "#56B4E9", "#009E73",
  "#F0E442", "#0072B2", "#D55E00", "#CC79A7"
)
```

Readers can also create palettes of their choice at [Color Brewer 2.0](https://colorbrewer2.org/). If you chose to create your own palette, we recommend having them included at the beginning of your R script. 

In order to use the colour blind friendly palettes that are not part of the `RColorBrewer` library, we need to use `scale_colour_manual` instead.
```{r visgg-colblind-black-plot, echo = TRUE, eval = TRUE, fig.cap="Colouring the points with the colour-blind palette. The colour is determined by cut.", comment = NA}
p1 + scale_colour_manual(values = cbbPalette)
```

### The Size Component

Another component of `geom_point()` is the size of the points. They can be changed by either setting or mapping. The size of the points is specified in millimeters. 

#### Setting the Size 

To change the size of all the points in the plot to 5mm,
```{r visgg-aes-size-set, echo = TRUE, eval = TRUE, fig.cap="The points in the scatterplot is set to 5mm. The  points are larger than the default size in the previous figures.", comment = NA}
p1 + geom_point(size = 5)
```

The points in Figure \@ref(fig:visgg-aes-size-set) are larger, which is as we hoped for. However, the colours of the points are the same. This contradicts our previous effort on mapping the colours of the points to `cut` and saved it as `p1` earlier. The reason is that `geom_point()` was called when we created `p1`, so when we called the `geom_point()` again to set the size, it overwrites the command to map the colours of the points.

In order to change the colour and size at the same time, we need to do so within the same `geom_point()`.
```{r visgg-aes-col-size, echo = TRUE, eval = TRUE, fig.cap="", comment = NA}
p + geom_point(aes(col = cut), size = 5)
```

#### Mapping the Size

Similar to mapping the colour component, the sizes of the points can be mapped to a variable. 
```{r visgg-aes-size-map, echo = TRUE, eval = TRUE, fig.cap="Mapping the size of the points based on the cut variable.", comment = NA}
p1 + geom_point(aes(size = cut))
```

Notice in Figure \@ref{fig:visgg-aes-size-map} that the points are black in colour but the legend still includes `cut`. This is because the mapping contradicts `p1` that was stored in such a way earlier:
```{r visgg-aes-size-exp2, echo = TRUE, eval = FALSE, comment = NA}
p1 <- p + geom_point(aes(col = cut))
```

The plot appears "incorrect" and there will be a lot of warnings, which is not printed here. In order to map both colour and size properly, we need to, again, specify the mapping of both colour and size at the same time.
```{r visgg-aes-col-size-map, echo = TRUE, eval = TRUE, fig.cap="Mapping the colour and size of the points to the cut variable.", comment = NA}
p + geom_point(aes(col = cut, size = cut))
```

### The Shape Component

Another component to consider is the shape of the points, which are identified using numbers. The default shape of points is circle. 
```{r visgg-aes-shape-list, fig.cap="Shapes", echo=FALSE, fig.cap="The shapes available in the package.", fig.align='center'}
knitr::include_graphics("data/images/visgg-ggplot2-shape-identity.png")
```

Suppose we want to set the shapes of the points to inverted triangles without changing the size and colour of the points, we start with the `p` object and make changes through `geom_point()`.
```{r visgg-aes-shape-invtri, echo = TRUE, eval = TRUE, fig.cap="Changing the points to inverted triangles", comment = NA}
p + geom_point(shape = 6)
```

To map the points to the `cut` of the diamonds and set the size of all the points to 5mm, 
```{r visgg-aes-shape-cut, echo = TRUE, eval = TRUE, comment = NA, fig.cap="Mapping the shape and setting the size of the points at the same time.", warning=FALSE}
p + geom_point(aes(shape = cut), size = 5)
```

You may have received a warning that the shape component is not recommended for ordinal variables such as `cut`. This is a recommendation. Usually, the shape component is used to better visualize nominal variables. It is the readers' choice to manipulate the shape component for better visual presentation.

To summarize,  we recommend including the choice of colour, size and shape in one call of `geom_point()` to minimize error. For example, 
```{r visgg-aes-summary, echo = TRUE, eval = FALSE, comment = NA, fig.cap="An example of mapping the colour, size and shape of the points to the cut variable.", warning=FALSE}
p + geom_point(aes(col = cut, size = cut, shape = cut))
```

## Geometrics

Geometric objects perform the actual rendering of the layer and control the type of plot that you created. The common ones are: 

- `geom_point()` produces scatterplots,
- `geom_line()` produces line graphs, and
- `geom_bar()` produces bar plots. 


### Line Graphs

Previously we have been drawing scatterplots to draw the relationship between `carat` and `price`. We used `geom_point()`. What happens if we used `geom_line()`? 
```{r visgg-geo-line-plain, echo = TRUE, eval = TRUE, fig.cap="A line graph to show relationship between diamond carat and price.", comment = NA}
p + geom_line()
```

#### Setting Colour, the Thickness and Type of Line

Similar to `geom_point()`, we can set the colour of the line to `red`.
```{r visgg-geo-line-col, echo = TRUE, eval = TRUE, fig.cap="Setting the colour of the line graph to red.", comment = NA}
p + geom_line(col = "red")
```

The thickness of the line can also be changed. It is set to 1 by default, but we can change it to any decimal of our choice. The larger the number, the thicker the line. 
```{r visgg-geo-line-size, echo = TRUE, eval = TRUE, fig.cap="Setting the thickness of the line to 1.5mm", comment = NA}
p + geom_line(size = 1.5)
```

The default type of line is a solid line, which is also coded as `1`. There are a total of [12 types of lines](http://sape.inf.usi.ch/quick-reference/ggplot2/linetype), in which seven of them can also be referred to using numbers 0 to 6 instead of the string values. We can change the solid line into `dashed` as follow:
```{r visgg-geo-line-type, echo = TRUE, eval = TRUE, fig.cap="The solid line has changed to dashed line.", comment = NA}
p + geom_line(linetype = "dashed")
```

#### Multiple Lines

To draw multiple lines, the points must be grouped by a variable. Otherwise, all the points will be connected by a single line. Hence, when we create the base layer, we need to specify the `group` that we want to group the points into. Usually, the grouping is based on a categorical variable. 

Suppose we are interested to draw the lines according to `cut`. 
```{r visgg-geo-lines-plain, echo = TRUE, eval = TRUE, fig.cap="Multiple lines (based on cut) are drawn in the same figure.", comment = NA}
p2 <- ggplot(my.diamonds, aes(x = carat, y = price, group = cut))
p2 + geom_line()
```

We can adjust the colour by the `group`.
```{r visgg-geo-lines-col, echo = FALSE, eval = TRUE, comment = NA}
ggplot(my.diamonds, aes(x = carat, y = price, group = cut)) +
  geom_line(aes(col = cut))
```

To map the colours of the lines to `cut`, there are two options:

- Option 1:
    ```{r visgg-geo-lines-col1, echo = TRUE, eval = FALSE, comment = NA}
ggplot(my.diamonds, aes(x = carat, y = price, group = cut)) +
  geom_line(aes(col = cut))
    ```
- Option 2:
    ```{r visgg-geo-lines-col2, echo = TRUE, eval = FALSE, comment = NA}
ggplot(my.diamonds, aes(
  x = carat, y = price, group = cut,
  col = cut
)) +
  geom_line()
    ```

Both options produce the exact same graph. However, we prefer Option 2 over Option 1 because we can manipulate the components of the line (and points) more efficiently when creating graphs that are more complex later on.

#### Lines with Points

It is no surprise that we can add points in a line graph:
```{r visgg-geo-line-points, echo = TRUE, eval = TRUE, comment = NA}
p + geom_line() + geom_point()
```

The appearance of the lines and points can be changed as discussed previously.

### Bar Plots

Bar plots are commonly used to graph categorical variables. 

Suppose we are interested in how the total price of diamonds is affected by the different `colour`. After laying down the base layer with `price` on the y-axis and `color` on the x-axis, we use the `geom_bar()` function to create the bars in the graph.
```{r visgg-geo-bar-plain, echo = TRUE, eval = TRUE, comment = NA}
ggplot(my.diamonds, aes(x = color, y = price)) +
  geom_bar(stat = "identity")
```

Notice that the x- and y-axes are similar to that of the scatterplots. The only difference is the use of `geom_bar()`. 

The colours of the bar can be mapped to the `color` variable by specifying the `fill` option.
```{r visgg-geo-bar-col, echo = TRUE, eval = TRUE, comment = NA}
ggplot(my.diamonds, aes(x = color, y = price, fill = color)) +
  geom_bar(stat = "identity")
```

## Others

It may be of interest to change

- x- and y-axes labels,
- title of the graph, and
- legends.

### Axes Labels

Similar to graphing in the `base` package, we can change the labels of the axes by adding the components as follows:

- x-axis: `xlab("name")`
- y-axis: `ylab("name")`

```{r visgg-lables, echo = TRUE, eval = TRUE, comment = NA}
p + geom_line(col = "red") +
  xlab("Price") + ylab("Carat")
```

### Title of the Graph

To add a title to the graph, we can use `ggtitle()`:
```{r visgg-title, echo = TRUE, eval = TRUE, comment = NA}
p + geom_line(col = "red") +
  xlab("Price") + ylab("Carat") +
  ggtitle("Relationship between price and carat")
```

The title is left-centered and can be adjusted through the "theme" layer which we will not cover here. In general, we prefer to not add a title to the graph because captions would be added in the final presentation of the data and results.

### Legends

There are two ways for changing the title and labels of the legend:

- modify the data frame directly, or
- use `scale_xxx_yyy()`. Refer [here](http://www.cookbook-r.com/Graphs/Legends_(ggplot2)/#with-fill-and-color) for the different combinations of `xxx` and `yyy`.

Suppose we want the legend to show the `cut` in different colours. Since the legend is related to the colour of the lines, `xxx` is `colour` and the variable is categorical, we set `yyy` to `discrete`:
```{r visgg-legend, echo = TRUE, eval = TRUE, comment = NA}
p + geom_line(aes(col = cut)) +
  scale_colour_discrete(
    name = "Cut of diamonds",
    breaks = c("Fair", "Good", "Very Good", "Premium", "Ideal"),
    labels = c("A", "B", "C", "D", "E")
  )
```





<!--chapter:end:mod_ggplot2.Rmd-->

# Introduction to Sample Size Determination

```{r, include = F}
knitr::opts_chunk$set(cache = T)
```

```{r, echo = F, warning = F, message = F}
source("common_functions.R")
library(bookdown)
# require("styler")
# styler::style_file()
```

\newpage
\setstretch{1.05}

*Author: Luke Hagar*

*Last Updated: October 17, 2022*

## Introduction

Sample size determination involves choosing the number of observations to include in a statistical sample. It is an important component of experimental design. Clients often visit the SSCR for statistical advice after the data collection process is complete. Sometimes, these clients have collected very small samples for their study, which can greatly complicate the analysis. In some cases, we may find major flaws in the study design and recommend that they redo the study. To avoid these situations, it is best to thoroughly consider sample size determination while designing a study. We note that some clients may have very limited funding and not be able to collect large samples, but considering sample size determination beforehand can help (i) set realistic expectations for the study or (ii) support the use of simpler statistical models when analyzing the data.

There are both practical and statistical considerations that inform sample size determination. Practical considerations for sample size determination include the cost of sampling each observation (sometimes called an experimental unit), the time associated with collecting each observation, and the convenience with which observations can be sampled. Statistical considerations include the statistical power (to be discussed) of the designed study and the precision of its relevant estimates. This chapter focuses on the statistical considerations for sample size determination, but it may be useful to also discuss practical considerations for sample size determination with clients.

### List of R Packages Used {#samp-rpackages}

In this chapter, we will be using the packages [**pwr**](https://cran.r-project.org/web/packages/pwr/), [**TOSTER**](https://cran.r-project.org/web/packages/TOSTER/), [**foreach**](https://cran.r-project.org/web/packages/foreach/), [**doParallel**](https://cran.r-project.org/web/packages/doParallel/), and [**doSNOW**](https://cran.r-project.org/web/packages/doSNOW/). We will also discuss the free sample size determination software [G$^*$Power](https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower.html).

```{r, message = FALSE}
library(pwr) ## load the required packages
library(TOSTER)
library(foreach)
library(doParallel)
library(doSNOW)
```

### Type I and Type II Errors {#samp-errors}

Most statistical studies involve testing a hypothesis of interest. When testing a hypothesis using statistical hypothesis tests, we can make two types of errors. To introduce these types of errors, we use a standard two-sample hypothesis test where we wish to test a one-sided hypothesis. We let $\mu_1$ be the mean response for the first group and $\mu_2$ be the mean response for the second group. The null and alternative hypotheses for this test are
$$H_0: \mu_1 \ge \mu_2 ~~~\text{vs.}~~~ H_A: \mu_1 < \mu_2.$$

We typically decide whether or not to reject the null hypothesis using $p$-values. If the $p$-value is smaller than a predetermined threshold $\alpha$, then we reject $H_0$; otherwise, we do not reject the null hypothesis. This type of standard hypothesis test does not allow us to accept the null hypothesis. In the frequentist framework, we consider $\mu_1$ and $\mu_2$ to be fixed, unknown values. Therefore, we do not known whether the statement $\mu_1 \ge \mu_2$ is true or false, but it *is* either true or false. 

A type I error occurs when we incorrectly reject the null hypothesis. For this example, that means that we reject $H_0$ when $\mu_1$ is indeed at least equal to $\mu_2$. A type II error occurs when we incorrectly do not reject the null hypothesis. For this example, this means that we do not reject $H_0$ when $\mu_1$ is less than $\mu_2$. Before discussing how to control the probability of making a type I or II error, it may be helpful to explain to the client what the consequences of making type I and II errors mean in the context of their experiment. Let's imagine that $\mu_1$ and $\mu_2$ represent the mean patient survival time for two types of cancer treatments. Treatment 1 (corresponding to $\mu_1$) is the standard existing treatment, whereas treatment 2 (corresponding to $\mu_2$) is a novel treatment. Clinicians might recommend replacing the standard treatment with treatment 2 if we obtain enough evidence to reject the null hypothesis $H_0: \mu_1 \ge \mu_2$. In this situation, a type I error would result in replacing the existing cancer treatment with an inferior one. A type II error would result in not replacing the existing cancer treatment with a superior one. The severity of the consequences associated with making type I and II errors should inform the probability with which clients are comfortable making them.

We typically control the probability of making a type I error by choosing a significance level $\alpha$ for the hypothesis test. We reject $H_0$ if the corresponding $p$-value is less than $\alpha$. For a point null hypothesis (e.g. $H_0: \mu = 0$), the $p$-value takes values of less than $\alpha$ with probability $\alpha$ when $H_0$ is true. Therefore, the significance level $\alpha$ is equal to the probability of making a type I error. Controlling the probability of making a type I error is slightly more complicated when the null hypothesis is not a point null hypothesis (e.g. $H_0: \mu \ge 0$); in this case, there is more than one value for $\mu$ such that the null hypothesis would be true. In these situations, the significance level $\alpha$ determines the size of the hypothesis test. For $H_0: \mu \ge 0$, let $\Omega_0 = \{\mu : H_0 ~\text{is true} \}$. The size of the test is then

$$\text{sup}_{\mu^* \in \Omega_0}~Pr( H_0 ~\text{is rejected} ~| ~\mu^* ~\text{is the true parameter value}).$$  
Thus, the size of the test is the maximum probability of making a type I error over the entire null hypothesis space $\Omega_0$. If we use a significance level of $\alpha$ for the $p$-value when conducting a hypothesis test, the size of the test is typically also equal to $\alpha$. In many situations, the size of the test is equal to the probability of making a type I error at some boundary point of $\Omega_0$. For this example, that means that our probability of making a type I error might be much lower than the size of the test for certain true $\mu$ values. For instance, the probability of making a type I error may be equal to $\alpha$ on the boundary $\mu = 0$, but it may be much less than $\alpha$ if the true, unknown value for $\mu$ is such that $\mu >> 0$. 

We typically control the probability of making a type II error when choosing the sample size for the study. We often let $\beta$ be the probability of making a type II error and define the power of a hypothesis test as 1 - $\beta$. Ideally, we want the power of a hypothesis test to be high. In general, the power of a hypothesis test can be made larger by increasing the sample size. The power of a test depends on the size of the effect that we would like to be able to detect. Effect size is discussed in more detail in Section \@ref(samp-effect-size). The effect size can generally been seen as the magnitude of the departure from the null hypothesis. Let's reconsider the null hypothesis $H_0: \mu_1 \ge \mu_2$. If $\mu_2 = \mu_1 + \epsilon$ for some small $\epsilon > 0$, then we may require a lot of data gather enough evidence to reject the null hypothesis. However, if $\mu_2 >> \mu_1$, we may not require much data to reject the null hypothesis. As such, effect size is an important consideration for sample size determination.

### Effect Size {#samp-effect-size}

Generally, the effect size can be seen as the magnitude of the departure from the null hypothesis. It follows that larger departures (effect sizes) should be easier to detect (i.e., we can detect them using smaller samples). We often assume that the data are approximately normal (an appropriate transformation may need to be applied). In this case, we can assume that the observed data $y_1,...,y_n$ are such that $Y_i \sim N(\mu, \sigma^2)$ for $i=1,...,n$ independently. It is often of interest to test the null hypothesis $H_0: \mu = \mu_0$. For this hypothesis test, the effect size would typically be given by
$$d = \frac{\mu - \mu_0}{\sigma}.$$
We discuss this further in Section \@ref(samp-common-models), but we often need to guess the value of $\sigma$ to conduct the sample size calculation. For a two-sided hypothesis test, the effect size $d$ can be positive or negative. Therefore, if the unknown, true value of $\mu$ is much larger or smaller than $\mu_0$, we should have greater probability of rejecting the null hypothesis for smaller samples. If we would instead like to conduct a one-sided hypothesis test, we need to more carefully consider the sign of the effect. For instance, we may want to test the null hypothesis $H_0: \mu \le \mu_0$. In this scenario, it is not seen as a departure from the null hypothesis when $\mu << \mu_0$. Therefore, we should only consider effect sizes $d > 0$ when selecting a sample size for the study.

The effect size may also be referred to as Cohen's $d$ [@cohen], particularly in scenarios where we want to compare two means using their standardized difference. In these scenarios, our null hypothesis of interest is typically $H_0: \mu_1 = \mu_2$, and the corresponding effect size is 
$$d = \frac{\mu_1 - \mu_2}{\sigma}.$$

For the purposes of sample size determination, it is often assumed that the variances for the two normal populations are unknown but the same for both groups. That is, we assume that $Y_{1j} \sim N(\mu_1, \sigma^2)$ for $j = 1,...,n_1$ and $Y_{2j} \sim N(\mu_2, \sigma^2)$ for $j = 1,...,n_2$. This assumption is typically made for two reasons. First, it simplifies the sample size calculation; we can sometimes compute the sample size that will achieve the desired power for the study using an analytical formula if we assume the variances are the same. We might need to resort to simulation-based sample size determination (discussed in Section \@ref(samp-sec-sim)) if we do not make this assumption. Second, it is often difficult enough for the client to estimate one value for $\sigma$. They may not have sufficient information to specify $\sigma_1$ and $\sigma_2$, different measures of variability for each group. If the client does have sufficient information to estimate $\sigma_1$ and $\sigma_2$, the pooled standard deviation could be used for sample size calculations:
$$\sigma = \sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}.$$

Although most clients visiting the SCCR may not have sufficient funding to obtain extremely large samples, it is important to note that we are able to detect very small effects when the sample size for the study is very large. As such, it is important to justify that the observed effect sizes are practically -- and not just statistically -- significant. For instance, we may want to test whether the average final grade in STAT 230 is the same as the average final grade in STAT 231. After collecting (potentially hundreds of) observations from each class, we may estimate the average final grades in STAT 230 and STAT 231 to be 75.1\% and 75.3\%, respectively. We may have collected enough data to conclude that the difference between the average final grades in the two classes is not exactly 0, but an increase of 0.2\% might not be very meaningful. Perhaps a 1\% difference in the average final grades is the smallest difference that would be practically important -- then this knowledge from the client should inform the effect size used in the sample size calculation. Clients should be prepared to justify that their observed effect sizes are meaningful in the context of their discipline. Simply observing a $p$-value that is less than the significance level $\alpha$ may be not be enough to guarantee publication. In recent years, certain journals (e.g. [Basic and Applied Psychology](https://www.tandfonline.com/toc/hbas20/current) and [Political Analysis](https://www.cambridge.org/core/journals/political-analysis)) have actually banned the use of $p$-values in their journals as a result of $p$-values being misused in certain situations [@wasserstein.lazar]. Certain journals are now placing more emphasis on precise effect estimation over $p$-values, which will be discussed briefly in Section \@ref(samp-precision). If applicable, it might be helpful for clients to consider the desired avenue for publication before designing their study. 

### Equivalence Testing {#samp-equivalence}

For certain analyses, clients may want to collect enough data to show that there is no substantial effect. In certain fields, clients may try to do this by computing the [post-hoc power](http://daniellakens.blogspot.com/2014/12/observed-power-and-what-to-do-if-your.html) of a study. For instance, if we did not reject the null hypothesis, we might try to use the observed standard deviation (not the value for $\sigma$ estimated prior to conducting the study) to determine what our "post-hoc" power is to detect effects of given sizes. This is not the recommended method for concluding that there is no substantial effect; however, certain journal editors or reviewers may request that a post-hoc power analysis be performed. Equivalence testing [@walker.nowacki] provides a more statistically sound method to conclude that there is no substantial effect. Equivalence testing requires its own sample size determination procedures.

Equivalence testing aims to conclude that the true effect size lies in the interval $(-\delta, \delta)$ for some $\delta > 0$; this value for $\delta$ is often referred to as the equivalence margin. For instance, a standard hypothesis test may involve the null hypothesis $H_0: \mu_1 = \mu_2$. Equivalence testing may instead consider the null hypothesis to be $H_0: \{\mu_1 - \mu_2 \le -\delta\}\cup\{\mu_1 - \mu_2 \ge \delta\}$. If we reject this hypothesis, we can conclude that $\mu_1 - \mu_2 \in (-\delta, \delta)$. The value of $\delta$ should typically be chosen by the client -- based on what would constitute a substantial effect size in the context of their comparison.

Equivalence testing is often carried out using a two one-sided test (TOST) procedure. For the previous example, we would first test the one-sided null hypothesis $H_{01}: \mu_1 - \mu_2 \le -\delta$ and then test the second one-sided null hypothesis $H_{01}: \mu_1 - \mu_2 \ge \delta$. Let's again assume that $Y_{1j} \sim N(\mu_1, \sigma^2)$ for $j = 1,...,n_1$ and $Y_{2j} \sim N(\mu_2, \sigma^2)$ for $j = 1,...,n_2$. If we reject both null hypotheses using one-sided $t$-tests with significance level $\alpha$, then we can reject the composite null hypothesis $H_0: \{\mu_1 - \mu_2 \le -\delta\}\cup\{\mu_1 - \mu_2 \ge \delta\}$ with significance level $\alpha$ as well. When conducting sample size determination for equivalence tests, we must specify (i) the estimates for quantities from the previous subsection ($d$, $\sigma$, etc.) and (ii) the equivalence margin $\delta$. @cohen provides some rough context for the standardized effect size $d$; $d = 0.2$, $d = 0.5$, and $d = 0.8$ respectively correspond to small, medium, and large effect sizes. The equivalence margin $\delta$ could be chosen to coincide with Cohen's advice in the absence of strong subject matter expertise. However, subject matter expertise from the client should be used to choose $\delta$ whenever possible. We often require larger sample sizes to achieve the desired power as the equivalence margin $\delta$ increases to 0.

### Precision-Based Sample Size Determination {#samp-precision}

Clients may be planning a study where the objective is not to show the absence or presence of an effect. Instead, they might wish to precisely estimate the size of an effect. A client may have this objective if alternatives to $p$-values are preferred in their field. They may also be conducting a follow-up study: previous work may be concluded that a non-null effect does exist, and they hope to estimate this effect with greater precision than the original study. In these situations, the sample size may be chosen to bound the length of a confidence interval for the true effect size. The most standard formula for an (approximate) confidence interval is 
$$\hat d \pm z_{1 - \alpha/2}\times s_d/\sqrt{n},$$
where $\hat d$ is the estimate for the effect size $d$ obtained from the data, $z_{1 - \alpha/2}$ is the $1 - \alpha/2$ quantile of the standard normal distribution, and $s_d$ is the estimated standard deviation of the effect size. In certain situations, it may be more appropriate to use a $t$-distribution with $\nu$ degrees of freedom instead of a normal distribution, or we may wish to consider a non-symmetric confidence interval. In this subsection, we focus only on the case for the simplest formula given above. In this case, the length of the confidence interval is 
$$2\times z_{1 - \alpha/2}\times s_d/\sqrt{n}.$$

To conduct this sample size determination calculation, we must choose a value $\alpha$ such that the confidence interval has the desired coverage $1 - \alpha$. We must also estimate $\sigma$, the value for $s_d$ to be used for the purposes of sample size determination. Given $\alpha$ and $\sigma$, the length of the confidence interval is a univariate function of $n$. Given a target length for the confidence interval $l$, we can choose the sample size to be
$$n = \frac{4 \times z_{1 - \alpha/2}^2 \times \sigma^2}{l^2}.$$

We note that the sample sizes returned by this approach may be substantially greater than those returned by the more standard sample size calculations to control type I error and power. For instance, if the true value of the effect is $d = 10$, we may need much less data to conclude that $d \ne 0$ than we would need to precisely estimate the effect. The sample size $n$ is a not a linear function of the target length $l$. That is, to decrease the length of the confidence interval by a factor of 10, we need to increase the sample size by a factor of $10^2 = 100$. However, these large sample sizes may be required if we want to estimate effects from the study with the desired level of precision.

## Sample Size Determination for Common Models {#samp-common-models}

### The pwr Package {#pwr-pckg}

In this subsection, we provide a brief overview of the [**pwr**](https://cran.r-project.org/web/packages/pwr/) package and its capabilities. The **pwr** package contains several types of functions. The first set of functions involve computing effect sizes for common analyses: `cohen.ES()`, `ES.h()`, `ES.w1()`, and `ES.w2()`. We will discuss the `cohen.ES()` function in more detail. This function returns the conventional effect sizes (small, medium, and large) for the tests available in the **pwr** package. This function has two parameters:

* `test` denotes the statistical test of interest. `test = "p"` is used for a hypothesis test to compare proportions. `test = "t"` is used to denote a $t$-test to compare means (one-sample, two-sample, or paired samples). `test = "r"` is used for a hypothesis test to compare correlations. `test = "anov"` is used to denote balanced one-way analysis of variance (ANOVA) tests. `test = "chisq"` is used to denote $\chi^2$ tests for goodness of fit or association between variables. Lastly, `test = "f2"` is used for a hypothesis test that considers the $R^2$ statistic from a general linear model.
* `size` denotes the effect size. It takes one of three values: `"small"`, `"medium"`, or `"large"`.

This function is helpful because you do not need to memorize the conventional effect sizes for the common statistical analyses listed above. The following R code shows what this function outputs for a medium effect corresponding to a balanced one-way ANOVA test.

```{r}
(medium.ANOVA <- cohen.ES(test="anov", size="medium"))
```

The effect size itself can be returned directly using the following code. This effect size can be used as an input value for many of the other functions in the **pwr** package.

```{r}
medium.ANOVA$effect.size
```

The remaining functions in the **pwr** package facilitate power analysis for common statistical analyses: `pwr.2p.test()`, `pwr.2p2n.test()`, `pwr.anova.test()`, `pwr.chisq.test()`, `pwr.f2.test()`, `pwr.norm.test()`, `pwr.norm.test()`, `pwr.p.test()`, `pwr.r.test()`, `pwr.t.test()`, and `pwr.t2n.test()`. The functions with `2n` in their names allow for power analysis will unequal sample sizes; the other functions force the sample sizes to be the same in each group. We will discuss some of these functions in greater detail later in this section (after introducing [G$^*$Power](https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower.html)). We note that power curves can be plotted for many of these analyses using the standard `plot()` function. This will also be discussed later in this section.

### G*Power Software

[G$^*$Power](https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower.html) is a free sample size determination software developed by Heinrich Heine University Düsseldorf in Germany. G$^*$Power is more comprehensive than the **pwr** package in R. Moreover, the [G$^*$Power user manual](https://uofwaterloo.sharepoint.com/:u:/r/sites/tm-arts-sccr-InternalGeneral/Shared%20Documents/Internal%20General/S22_Sample_Size/GPower_Manual.url?csf=1&web=1) is more detailed than the documentation for the **pwr** package in R. Although neither G$^*$Power nor the **pwr** package offer complete functionality for sample size determination, they offer a good starting point for common statistical analyses. The interface for G$^*$Power might be easier for clients to use, particularly those who do not have much experience with R. However, if the client is already familiar with with R and their desired analysis is supported by the **pwr** package, then the **pwr** pacakge may be the more suitable choice.

G$^*$Power supports statistical power analyses for many different statistical tests of the $F$-test family, the $t$-test family, $\chi^2$-test family, $z$-test family, and even some exact and nonparametric tests. G$^*$Power provides both effect size calculators and graphics options. G$^*$Power offers five different types of statistical power analysis. First, it supports a priori analysis, where the sample size $N$ is computed as a function of power level $1 - \beta$, significance level $\alpha$, and the to-be-detected population effect size. This is the most common type of power analysis. Second, it supports compromise power analysis, where both $1 - \alpha$ and $\beta$ are computed as functions of the effect size, $N$, and an error probability ratio $q = \beta/\alpha$. Third, it supports criterion power analysis, where the significance level $\alpha$ is chosen to obtain the desired power $1 - \beta$ for a given effect size and $N$. Fourth, it supports post-hoc poweranalysis, where $1 - \beta$ is computed as a function of $\alpha$, the observed population effect size and $N$. This type of power analysis may not be recommended. Finally, it supports sensitivity power analysis, where the population effect size is computed as a function of $\alpha$, power $1 - \beta$, and $N$. Sensitivity analysis can be helpful to gauge how sensitive the sample size recommendation is to the effect size (which is to be observed). In situations where the sample size recommendation is very sensitive to the to-be-observed population effect size, it might be best to consider a conservative sample size. This chapter focuses on a priori power analysis.

With G$^*$Power, one can use dropdown menus to choose the type of power analysis (typically a priori), the family of test (e.g. $F$-test, $t$-test, etc.), and the statistical model (e.g. $t$-test to compare correlations, $t$-test to compares means, etc.). For a priori power analyses, one will typically need to choose an effect size, $\alpha$, and desired power $1 - \beta$ to complete the sample size calculation and return a suitable sample size. We discuss how to do this for several common statistical models in the following subsections.

### Sample Size Determination for Balanced One-Way ANOVA {#samp-ex-ANOVA}

In this subsection, we focus on the fixed effects one-way ANOVA test. It tests whether there are any differences between the means $\mu_i$ of $k \ge 2$ normally distributed random variables each with common variance $\sigma$. As such, one-way ANOVA can be viewed as an extension of the two group $t$-test for a difference of means to more than two groups.

The null hypothesis is that all $k$ group means are identical 
$$H_0: \mu_1 = \mu_2 = ... = \mu_k$$
The alternative hypothesis $H_A$ states that at least two of the $k$ means differ. That is, $H_A$ is such that $H_A : \mu_i \ne \mu_j$, for at least
one pair $i \ne j$ with $1 \le i, j \le k$.

For one-way ANOVA, the effect size is $f = \sigma_m/\sigma$, where $\sigma_m$ is the standard deviation of the group means $\mu_i$ for $i = 1, ..., k$ and $\sigma$ the common standard deviation *within* each of the $k$ groups. Therefore, $f$ may be difficult to choose directly; however, it can be computed directly if we know the number of groups $k$, the common within-group standard deviation $\sigma$ and a table of $(\mu_i, n_i)$ values for $i = 1,...,k$. These numbers may be easier to estimate for clients. G$^*$Power facilitates this process using their application, and this process is described in more detail on page 24 in the [G$^*$Power user manual](https://uofwaterloo.sharepoint.com/:u:/r/sites/tm-arts-sccr-InternalGeneral/Shared%20Documents/Internal%20General/S22_Sample_Size/GPower_Manual.url?csf=1&web=1). After users enter the number of groups $k$ and common standard deviation $\sigma$, an Excel-type table appears where users can enter the anticipated means $\mu_i$ and sample sizes $n_i$ for each group $i = 1, ..., k$. G$^*$Power then returns the corresponding effect size $f$. Users are also free to use the conventional effect sizes for one-way ANVOA as determined by @cohen: $f =$ 0.1 for a small effect, 0.25 for a medium effect, and 0.4 for a large effect. 

We now consider the following example. We suppose that we would like to compare 10 groups, and we expect to observe a "medium" effect size ( $f$ = .25). We now use the `pwr.anova.test()` function in the **pwr** package to determine how many observations we need in each group with $\alpha$ = 0.05 to achieve a power of 0.95. This example is also provided in the [G$^*$Power user manual](https://uofwaterloo.sharepoint.com/:u:/r/sites/tm-arts-sccr-InternalGeneral/Shared%20Documents/Internal%20General/S22_Sample_Size/GPower_Manual.url?csf=1&web=1), so we will be able to compare the results from the two programs. The following R code conducts the sample size determination calculation for this example. 

```{r}
(p.anova <- pwr.anova.test(k=10, f = 0.25, power=0.95, sig.level = 0.05))
```

After rounding the sample size up to the nearest integer, we determine that we require 39 observations per group for this example. This is the same sample size recommendation returned by G$^*$Power. For a given sample size determination scenario, we can return a power curve using the standard `plot()` function in R (on an object to which the power calculation was saved) as demonstrated below in Figure \@ref(fig:pwr1).

```{r pwr1, fig.cap="Visualization of power curve for one-way ANOVA example", fig.align="center"}
plot(p.anova)
```

Figure \@ref(fig:pwr1) depicts the power of the one-way ANOVA test as a function of the sample size per group $n$. Similar graphics can also be produced in G$^*$Power. Although G$^*$Power allows the effect size $f$ to be estimated using a table of $(\mu_i, n_i)$ values where the $n_i$ values are not forced to be equal for all $i = 1, ...,k$, G$^*$Power provides a common sample size recommendation for each group. The one-way ANOVA is called a balanced one-way ANOVA when $n_1 = ... = n_k = n$. Finally, we note that neither the **pwr** package nor G$^*$Power provide functionality for sample size determination for one-way ANOVA when the within-group variance is not the same for all groups. We discuss how to conduct sample size calculations for such scenarios in Section \@ref(samp-sec-sim).

### Sample Size Determination for Multiple Regression Model {#samp-ex-mlr}

In this subsection, we focus on an omnibus $F$-test for multiple regression where we consider the deviation of $R^2$ (the coefficient of determination) from 0. The multiple linear regression model explains the relationships between a dependent variable $Y$ and $p$ independent variables $X_1$, ..., $X_p$. The type of multiple linear regression model explored in this subsection assumes that the independent variables are fixed. Thus, we consider a model for the response $Y$ that is conditional on the values of the independent variables. Therefore, we assume that given $N$ ($\boldsymbol{Y}$, $\boldsymbol{X}$)observations,
$$\boldsymbol{Y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon},$$
where $\boldsymbol{X} = [1, \boldsymbol{X}_1, ..., \boldsymbol{X}_p]$ is a $N \times (p + 1)$ matrix of a constant term and fixed, known predictor $\boldsymbol{X}_i$. Moreover, $\boldsymbol{\beta}$ is a column vector of $p + 1$ regression coefficients. The column vector $\boldsymbol{\varepsilon}$ of length $N$ contains the error terms such that $\varepsilon_i \sim N(0, \sigma^2)$ for $i = 1, ..., N$.

G$^*$Power supports power analyses for the test that the proportion of variance of the dependent variable $Y$ explained by a set of predictors $B = \{1, ..., p\}$, denoted $R^2_{Y\cdot B}$, is equal to 0. The corresponding null and alternative hypotheses are
$$H_0: R^2_{Y\cdot B} = 0 ~~~\text{vs.}~~~ H_A: R^2_{Y\cdot B} > 0.$$
Under the null hypothesis, the appropriate test statistic follows an $F$ distribution with $p$ degrees of freedom in the numerator and $N - (p+1)$ degrees of freedom in the denominator.

For this procedure, the effect size $f^2$ is such that $f^2 = V_S/V_E$, where $V_S$ is the proportion of variance explained by the set of predictors $B$, and $V_E$ is the proportion of residual variance not explained by the predictors. That is, $V_S + V_E = 1$. For the multiple linear regression scenario given above, the proportion of variance explained by the set of predictors $B$ is given by $R^2_{Y\cdot B}$ and the residual variance is given by $V_E = 1 - R^2_{Y\cdot B}$. It follows that
$$f^2 = \frac{R^2_{Y\cdot B}}{1 - R^2_{Y\cdot B}}.$$

G$^*$Power provides functionality to choose an effect size $f^2$ using estimated correlations between the independent variables and the estimated correlations between the dependent variable $Y$ and each independent variable. After users enter the number of independent variables $p$ in their model, two Excel-type table appear. The first table is of dimension $p \times 1$ and allows users to estimate a correlation coefficient between the response variable $Y$ and each predictor variable. The second table is a matrix of dimension $p \times p$, where users can estimate correlation coefficients between each pair of independent variables $(X_i, X_j)$ such that $i < j$. We note that the correlation between $X_i$ and itself is 1 and that the correlation coefficient for the pair of variables $(X_i, X_j)$ is symmetric in the roles of $i$ and $j$. After the user inputs these estimated values into the application, G$^*$Power returns an effect size $f^2$. This process is explained in more detail on pages 33 and 34 of the [G$^*$Power user manual](https://uofwaterloo.sharepoint.com/:u:/r/sites/tm-arts-sccr-InternalGeneral/Shared%20Documents/Internal%20General/S22_Sample_Size/GPower_Manual.url?csf=1&web=1). Users are also free to use the conventional effect sizes for the omnibus test for $R^2$ with multiple linear regression as determined by @cohen: $f^2 =$ 0.02 for a small effect, 0.15 for a medium effect, and 0.35 for a large effect. 

We now consider the following example. We suppose that we would like to consider a regression model with 5 independent variables, and we have reason to expect an effect size of $f$ = 1/9. This is somewhere between a small and medium effect size. We now use the `pwr.f2.test()` function in the **pwr** package to determine the power that we have to detect this effect when $\alpha$ = 0.05 and we have $N =$ 95 observations. This example is also provided in the [G$^*$Power user manual](https://uofwaterloo.sharepoint.com/:u:/r/sites/tm-arts-sccr-InternalGeneral/Shared%20Documents/Internal%20General/S22_Sample_Size/GPower_Manual.url?csf=1&web=1), so we will be able to compare the results from the two programs. The following R code conducts the sample size determination calculation for this example. 

```{r}
(p.f2 <- pwr.f2.test(u=5, v = 89, f2 = 1/9, sig.level = 0.05))
```

For the `pwr.f2.test()` function, `u` is the degrees of freedom in the numerator, `v` is the degrees of freedom in the denominator, and `f2` is the effect size. With these inputs, we estimate the power of the appropriate $F$-test to be 0.674. This is the same estimated power returned by G$^*$Power. The **pwr** package does not faciliate plotting power curves for the `pwr.f2.test()` function. This relatively simple multiple linear regression example exhausts the **pwr** package's sample size determination capabilities. G$^*$Power provides much more functionality for sample size determination with regression models, including sample size calculations for comparing a single regression coefficient to a fixed value, sample size calculations for comparing the equality of two regression coefficients, and sample size calculations for considering the increase in the $R^2$ value that arises from using a more complicated multiple linear regression model. These scenarios are described in the [G$^*$Power user manual](https://uofwaterloo.sharepoint.com/:u:/r/sites/tm-arts-sccr-InternalGeneral/Shared%20Documents/Internal%20General/S22_Sample_Size/GPower_Manual.url?csf=1&web=1).

### Sample Size Determination to Compare Two Means (Unknown, Common Variance) {#samp-ex-t}

In this subsection, we focus on a third and final scenario for sample size determination with the **pwr** package and G$^*$Power. For this scenario, we consider a $t$-test to gauge the equality of two independent populations means $\mu_1$ (corresponding to group 1) and $\mu_2$ (corresponding to group 2). The data consist of two samples of size $n_1$ and $n_2$ from two independent and normally distributed populations. For this test, we assume that the true standard deviations in the two populations are unknown and will be estimated from the data. The null and alternative hypotheses of this $t$-test are
$$H_0: \mu_1 = \mu_2 ~~~\text{vs.}~~~ H_A: \mu_1 \ne \mu_2.$$
We use a two-sided test if there is no restriction on the sign of the deviation of the alternative hypothesis $H_A$. Otherwise, we use the one-sided test.

As mentioned earlier, the effect size in this scenario is 
$$d = \frac{\mu_1 - \mu_2}{\sigma},$$
where $\sigma$ is the common standard deviation for the two groups of normal data. @cohen suggested using $d =$ 0.2 for a small effect, 0.5 for a medium effect, and 0.8 for a large effect. However, the effect size $d$ for this situation is more interpretable than the effect sizes for the previous two scenarios considered. As such, clients may have substantial enough information to choose an effect size for the sample size calculation.

We now consider the following example. We suppose that we would like to compare two means, and we expect to observe a "medium" effect size ( $d$ = 0.5). We want to perform a two-sided hypothesis test to consider the equality of $\mu_1$ and $\mu_2$. We now use the `pwr.t.test()` function in the **pwr** package to determine how many observations we need in each group with $\alpha$ = 0.05 to achieve a power of 0.8. The following R code conducts the sample size determination calculation for this example. 

```{r}
(p.t <- pwr.t.test(d=0.5, power=0.8, sig.level = 0.05, type = "two.sample", 
                   alternative = "two.sided"))
```

For the `pwr.t.test()` function, `type` can take a value of either `"two-sample"`, `"one-sample"`, or `"paired"`. Moreover, `alternative` can take a value of `"two.sided"`, `"less"`, or `"greater"` -- where `"less"` and `"greater"` should correspond to negative and positive values of `d`, respectively. After rounding the sample size up to the nearest integer, we determine that we require 64 observations per group for this example. For this example, we can return a power curve using the standard `plot()` function in R as demonstrated below in Figure \@ref(fig:pwr3).

```{r pwr3, fig.cap="Visualization of power curve for two-sample t-test example", fig.align="center"}
plot(p.t)
```

Figure \@ref(fig:pwr3) depicts the power of the two-sample $t$-test as a function of the sample size per group $n$. Similar graphics can also be produced in G$^*$Power. The two-sample $t$-test functions in the **pwr** package force the standard deviation $\sigma$ in the two groups to be the same. G$^*$Power does allow for sample size calculations where the standard deviations are not the same for the two groups (i.e., $\sigma_1 \ne \sigma_2$) for the case when $n_1 = n_2$. However, this sample size determination calculation still assumes that [Student's *t*-test](https://en.wikipedia.org/wiki/Student%27s_t-test) will be used, and this test assumes that the standard deviation is the same in both groups. Therefore, this sample size calculation facilitated by G$^*$Power is an approximate calculation. When $\sigma_1 \ne \sigma_2$, it is more appropriate to use [Welch's *t*-test](https://en.wikipedia.org/wiki/Welch%27s_t-test). Most free sample size determination software solutions do not offer this functionality, but we discuss how to conduct sample size calculations for this particular scenario in Section \@ref(samp-sec-sim). We note that both the **pwr** package in R and G$^*$Power offer more sample size determination functionality than explored in the previous three examples. More details about this additional functionality can be found in the documentation for these software solutions.

### The TOSTER Package {#samp-TOSTER}

To conclude this subsection, we discuss the [**TOSTER**](https://cran.r-project.org/web/packages/TOSTER/) package, which facilitates sample size calculations for equivalence tests when the TOST procedure is carried out using two one-sided $t$-tests. This package facilitates sample size determination for two-sample, one-sample, and paired $t$-tests, but we will focus on the two-sample case in this subsection. For the two-sample case, the **TOSTER** package requires that we make the assumption that the standard deviation $\sigma$ is the same in both groups. For this scenario and a given equivalence margin $\delta$, our null and alternative hypotheses are
$$H_0: \{\mu_1 - \mu_2 \le -\delta\}\cup\{\mu_1 - \mu_2 \ge \delta\} ~~~\text{vs.}~~~ H_A: -\delta < \mu_1 - \mu_2 < \delta.$$

For this sample size determination calculation, we need to choose a values for the equivalence margin $\delta$, the significance level $\alpha$, and the power $1 - \beta$; we must also estimate a value for the common standard deviation $\sigma$ and estimate a value for $\mu_1 - \mu_2$. The `power_T_tost()` function allows us to specify the true effect via the raw (unstandardized) effect. The `power_t_TOST()` function has the following arguments:

* `delta`: the value for `delta` is *not* the value specified for the equivalence margin $\delta$. Instead, it is the value that we anticipate to observe for $\mu_1 - \mu_2$. We can specify `delta` using the anticipated difference between $\mu_1$ and $\mu_2$. It is important to specify this difference because the power of the equivalence test is going to depend on how close the true value of $\mu_1 - \mu_2$ is to either endpoint of the interval $(-\delta, \delta)$. If we anticipate that $\mu_1 = \mu_2$, we may need less data to conclude practical equivalence than if $\mu_1 - \mu_2$ is close to $-\delta$ or $\delta$.
* `sd`: the estimated value for the common standard deviation for the two groups.
* `low_eqbound`: the lower bound of the region of equivalence. This is typically $-\delta$, but the `power_T_tost()` allows for this interval to be asymmetric around 0.
* `high_eqbound`: the upper bound of the region of equivalence. This is typically $\delta$, but the `power_T_tost()` allows for this interval to be asymmetric around 0.
* `alpha`: the significance level for the test.
* `power`: the desired power of the equivalence test.

We now consider the following example. We suppose that we would like to compare two means, and we anticipate the difference between the two means to be 0.5. We anticipate that each group of data has a standard deviation of 5. Moreover, we define the equivalence margin to be 2 (i.e., it is not of practical importance if the means of the two groups differ by at most 2 in absolute value). We now use the `power_t_TOST()` function to determine how many observations we need in each group with $\alpha$ = 0.05 to achieve a power of 0.8.  The following R code conducts the sample size determination calculation for this example. 

```{r}
(p.TOST <- power_t_TOST(delta = 0.5, sd = 5, low_eqbound = -2, high_eqbound = 2, 
                        alpha = 0.05, power = 0.8))
```

After rounding the sample size up to the nearest integer, we determine that we require 141 observations per group for this example. The **TOSTER** function does not support the plotting of power curves for equivalence tests. These curves must be plotted manually, as discussed in Section \@ref(samp-sec-sim). The **TOSTER** package only supports basic sample size calculations for equivalence testing, but it is an accessible starting point in terms of software that is freely available.

## Simulation-Based Sample Size Determination {#samp-sec-sim}

### Computing a Single Power Estimation via Simulation {#samp-sim-sing}

If the model that we intend to use to analyze the data from the study is not supported by the **pwr** package in R or G$^*$Power, we may need to resort to simulation-based sample size determination. Simulation-based sample size determination involves specifying distributions for the data and generating samples of a given size $n$. These samples of size $n$ can typically be generated using built-in R functions -- such as `rnorm()`, `rexp()`, `rpois()`, or `rgamma()`. For this simulated data, we conduct the hypothesis test of interest and record whether or not we reject the null hypothesis for a specified significance level $\alpha$. We can repeat this process many times, and the proportion of simulations for which we reject the null hypothesis provides an estimate for the power of the test given the distribution(s) chosen for the data, the significance level $\alpha$, and the sample size $n$. 

Simulation-based sample size determination can be useful in situations where heteroscedasticity is present (i.e., we have multiple groups of data, and the variance within each group is not the same). In these situations, we may not be able to compute the power of the statistical test using analytical formulas. Moreover, simulation-based sample size determination can also be useful when we would like to use nonparametric hypothesis tests. The **pwr** package does not provide any functionality for samples size calculations with nonparametric tests, and G$^*$Power offers very limited functionality.

We illustrate how to conduct simulation-based sample size determination using the following example. Let's assume that we want to conduct a two-sample $t$-test, but we have reason to believe that the standard deviations from the two groups are not the same. Let's assume that we want to conduct a two-sided hypothesis test. Let's further assume that we believe that $Y_{1j} \sim N(0, 1)$ for $j = 1, ..., n_1$ and that $Y_{2j} \sim N(1, 2)$ for $j = 1, ..., n_2$. As such, we would anticipate using Welch's $t$-test to conduct the hypothesis test. The Student's two-sample $t$-test leverages a $t$-distribution with $n_1 + n_2 - 2$ degrees of freedom. However, under the null hypothesis $H_0: \mu_1 = \mu_2$ the Welch's two-sample $t$-test has a $t$-distribution with the degrees of freedom given by
$$df^* = \dfrac{\left(\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}\right)^2}{\frac{s_1^4}{n_1^2(n_1 -1)} + \frac{s_2^4}{n_2^2(n_2 -1)}},$$
where $s_1^2$ and $s_2^2$ are the observed sample variance for the observations from groups 1 and 2. That is, we do not know the degrees of freedom for the test statistic a priori. This can make it difficult to compute the power of the $t$-test analytically. The following R code (with comments) shows how to estimate the power of Welch's $t$-test in this scenario for $n_1 = n_2 = 50$ and significance level $\alpha = 0.05$ via simulation. This example assumes that the study design is balanced (i.e., $n_1 = n_2$), but this is certainly not a requirement for simulation-based sample size determination.

```{r}
## choose the number of simulation repetitions
n_sim <- 1000

## set the significance level for the test
alpha <- 0.05

## set the sample size for each group
n1 <- 50
n2 <- 50

## set the anticipated parameters for each group
mu1 <- 0; sigma1 <- 1
mu2 <- 1; sigma2 <- 2

set.seed(1)
## initialize vector to output results of hypothesis test for simulated data
reject <- NULL
for (i in 1:n_sim){
  ## randomly generate simulated data from both groups
  y1 <- rnorm(n1, mu1, sigma1)
  y2 <- rnorm(n2, mu2, sigma2)
  
  ## conduct Welch's test (i.e., set var.equal = FALSE, which is the default)
  p.sim <- t.test(y1, y2, alternative = "two.sided", var.equal = FALSE)$p.value
  
  ## turn results into binary vector (1 if we reject null hypothesis; 0 otherwise)
  reject[i] <- ifelse(p.sim < alpha, 1, 0)
}

mean(reject)
```

Therefore, we estimate that Welch's $t$-test has power of 0.875 to reject the null hypothesis $H_0: \mu_1 = \mu_2$ when $Y_{1j} \sim N(0, 1)$ for $j = 1, ..., n_1$ and that $Y_{2j} \sim N(1, 2)$ for $j = 1, ..., n_2$ for $n_1 = n_2 = 50$ and a signficance level of $\alpha = 0.05$. For this simulation, we chose `n_sim` = 1000. This value ensures that the code runs quickly when compiling this document. We would likely want to use a larger value (at least `n_sim` = 10000) in practice. If we use small values for `n_sim`, then the estimated power will not be very precise. If $\widehat{pwr}$ is a given estimate for the power of a hypothesis test in a given scenario, then the standard error of this estimate is given by 
$$\sqrt{\frac{\widehat{pwr}(1 - \widehat{pwr})}{n_{sim}}}.$$
Therefore, if we choose `n_sim` = 10000, then the standard error of the estimate for power is at most 0.005 (this maximum is obtained when $\widehat{pwr} =$ 0.5).

### Estimating a Power Curve via Simulation {#samp-est-curve}

Both the **pwr** function in R and G$^*$Power facilitate the creation of power curves in certain scenarios. These power curves can also be estimated via simulation, although this may be a computationally intensive process. The following block of code takes the power estimation code from the previous code block. This function `estPower()` has eight inputs: the number of simulation repetitions `n_sim`, the sample sizes for each group `n1` and `n2`, the anticipated parameters of the normal distribution for group 1 `mu1` and `sigma1`, the anticipated parameters of the normal distribution for group 2 `mu2` and `sigma2`, and the significance level `alpha`. This function can generally be used to conduct power analysis for two-sided Welch $t$-tests.

```{r}
## put the previous code in functional form
estPower <- function(n_sim, n1, n2, mu1, sigma1, mu2, sigma2, alpha){
  ## initialize vector to output results of hypothesis test for simulated data
  reject <- NULL
  for (i in 1:n_sim){
    ## randomly generate simulated data from both groups
    y1 <- rnorm(n1, mu1, sigma1)
    y2 <- rnorm(n2, mu2, sigma2)
  
    ## conduct Welch's test (i.e., set var.equal = FALSE, which is the default)
    p.sim <- t.test(y1, y2, alternative = "two.sided", var.equal = FALSE)$p.value
  
    ## turn results into binary vector (1 if we reject null hypothesis; 0 otherwise)
    reject[i] <- ifelse(p.sim < alpha, 1, 0)
  }
  return(mean(reject))
}
```

The following R code demonstrates how to estimate and plot a power curve for this scenario. To estimate the power curve, we need to choose sample sizes at which to estimate the power of Welch's $t$-test to reject the null hypothesis. For this example, we will restrict the sample size to be balanced for the two groups ($n_1 = n_2$). We choose the following 11 sample sizes at which to estimate the power of Welch's test: 2, 5, 8, 10, 15, 20, 30, 40, 50, 60, and 75. The estimated power curve is plotted in Figure \@ref(fig:sim1).

```{r sim1, fig.cap="Visualization of power curve estimated via simulation", fig.align="center"}
## choose the number of simulation repetitions
n_sim <- 1000

## set the significance level for the test
alpha <- 0.05

## set the sample size for each group (multiple values)
n1 <- c(2,5,8,10,15,20,30,40,50,60,75)
n2 <- c(2,5,8,10,15,20,30,40,50,60,75)

## set the anticipated parameters for each group
mu1 <- 0; sigma1 <- 1
mu2 <- 1; sigma2 <- 2

set.seed(2)
## initialize vector to output results of hypothesis test for simulated data
pwer <- NULL
for (j in 1:length(n1)){
  ## use helper function to estimate power
  pwer[j] <- estPower(n_sim, n1[j], n2[j], mu1, sigma1, mu2, sigma2, alpha) 
}

plot(n1, pwer, type = "l", col = "firebrick", ylim= c(0,1),
     main = "Power curve for two-sided Welch test example", ylab = "Power",
     xlab = "Sample size (per group)")
```

We note that the R code in this subsection could be modified to facilitate simulation-based sample size determination in other scenarios. The code above runs relatively quickly but the estimated power curve is not particularly smooth. To improve this, we may want to estimate the power of Welch's test for more sample sizes than just the 11 that were considered above. Moreover, we would likely want to use a value of `n_sim` that is at least 10000 per estimate on the power curve. This suggests that estimating power curves via simulation can be a computationally intensive process. We discuss how to speed up this process in Section \@ref(samp-parallel).

### Parallelization for Simulation-Based Sample Size Determination {#samp-parallel}

Parallelization can help make computing power curves or power estimates a faster process when one has access to a computer with multiple cores. Parallel computation structures the computing process such that many calculations or processes are carried out simultaneously. A large problem (computing a power curve) can often be divided into smaller ones (computing the various power estimates that comprise the power curve). These smaller problems can then be solved at the same time. The parallelization solution provided in this subsection leverages the [**foreach**](https://cran.r-project.org/web/packages/foreach/), [**doParallel**](https://cran.r-project.org/web/packages/doParallel/), and [**doSNOW**](https://cran.r-project.org/web/packages/doSNOW/) packages in R. The following R code shows how to estimate the power curve from Section \@ref(samp-est-curve) using parallel computing.

```{r, eval = FALSE}
## load the required packages in this order
require(foreach)
require(doParallel)
require(doSNOW)

## set up the multiple cores for parallelization
## cores[1] returns the number of cores on your machine;
## this command makes a parallel computing environment with cores[1]-1 cores;
## this frees up a core to do other processes (do not assign more than cores[1]-1 cores).
cores=detectCores()
cl <- makeSOCKcluster(cores[1]-1)

## determine the number of parallel processes (the number of points on the power curve)
n_loop <- length(n1)

## choose the number of simulation repetitions
n_sim <- 1000

## set the significance level for the test
alpha <- 0.05

## set the sample size for each group (multiple values)
n1 <- c(2,5,8,10,15,20,30,40,50,60,75)
n2 <- c(2,5,8,10,15,20,30,40,50,60,75)

## set the anticipated parameters for each group
mu1 <- 0; sigma1 <- 1
mu2 <- 1; sigma2 <- 2

## this code sets up a progress bar; this is helpful if you have less than n_loop
## cores available (you can see the proportion of the smaller tasks that have been completed)
registerDoSNOW(cl)
pb <- txtProgressBar(max = n_loop, style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress = progress)

## the foreach() function facilitates parallelization (described below)
pwer2 <- foreach(j=1:n_loop, .combine="c", .packages = c("foreach"),
                               .options.snow=opts) %dopar% {
                                 set.seed(j)
                                 
                                 ## function outputs the estimated power for each
                                 ## point on the curve (from previous code block)
                                 estPower(n_sim, n1[j], n2[j], mu1, sigma1, mu2, sigma2, alpha)
                               }
```

The previous code block can be used to reproduce the estimated power curve from earlier (just replace `pwer` with `pwer2` in the plot command). The `foreach()` function faciliates the parallelization for this example. We discuss several of its important arguments below:

* `j`: this input should take the form `1:n_loop`, where `n_loop` is the number of tasks that must be computed in parallel.
* `.combine`: this argument denotes how the objects returned by each parallel task should be combined. Specifying `"c"` will concatentate these results into a vector, and using `cbind` or `rbind` can be used to combine vectors into a matrix. For this example, each parallel task returns a single power estimate, so we selected `"c"`.
* `.packages`: this argument should be a character vector denoting the packages that the parallel tasks depend on. For this situation, we do not need the **foreach** package to compute the power estimates, but this shows the format that this argument must take if we do have any dependencies for the parallel tasks.
* `options.snow`: this argument sets up the progress bar. As long as `n_loop` is modified as necessary, we do not need to change any other code associated with the progress bar to modify this code for a different example.
* `.errorhandling`: this argument is set to `stop` by default. This means that if one of the cores encounters an error message when computing a smaller task, the execution of all tasks is stopped. We recommend using this default setting so that you are alerted to the issue and can investigate potential problems with the code.

If you have a very computationally intensive sample size calculation, you can run your code on the [MFCF servers](https://uwaterloo.ca/math-faculty-computing-facility/services/research-and-staff-windows-servers) for graduate students, faculty, and staff in the Math Faculty. You should have up to 72 cores available when using the MFCF servers, which is likely more computing power than is available on a desktop computer.

Finally, the above code block computed the power estimate for each point on the power curve using a regular for loop. Each of the estimates on the power curve were computed in parallel. If it takes a very long time to compute a single power estimate, then you may want to compute each power estimate in parallel and iterate through all power estimates on the curve using a regular for loop. This could be done by modifying the code block above. We could replace `estPower()` with a function that returns a binary result to denote whether the null hypothesis was rejected for a single simulation run. We would want to then specify `j=1:n_sim` for the `foreach()` function call, and then we would wrap this `foreach()` function call in a regular for loop from `i = 1:n_loop`. We note that it is certainly not necessary to parallelize a sample size calculation, but it can be helpful if the calculation is very computationally intensive.

## Beyond Sample Size Determination {#samp-beyond}

Sound experimental design does not end with sample size determination. It can be helpful to discuss concepts that are related to sample size determination with the client. For instance, maybe the client has decided that they would like to collect 50 observations per group for the two groups in their study. In this situation, it would be a good idea to discuss [randomization](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3136079/) with the client. It is not ideal to simply assign the first 50 observations to the first treatment and the last 50 observations to the second treatment.

There may also be situations where a client might want to collect a smaller sample than recommended and then decide whether or not to collect a larger sample based on the results from analyzing the smaller sample. This is related to the statistical concept of [peeking](https://gopractice.io/data/peeking-problem/), where we monitor the data as they are observed and decide whether to continue or stop the experiment. It is not ideal to observe the data in stages, but it should be disclosed as part of the analysis that the data were not observed all at once if the data from both samples is used as part of the study.

<!--chapter:end:mod_sample_size.Rmd-->

# (APPENDIX) Appendix {-} 

# General Guidelines

*Author: Martin Lysy*

*Last Updated: Nov 17, 2020*

--- 

## Resources

- **R Markdown:** The  [**bookdown** e-book](https://bookdown.org/yihui/bookdown/) the [Definitive R Markdown Guide](https://bookdown.org/yihui/rmarkdown/) and the [R Markdown Cookbook](https://bookdown.org/yihui/rmarkdown-cookbook/).

- **LaTeX:** <a name="genguide-res:latex"> [Basic](https://en.wikibooks.org/wiki/LaTeX/Mathematics) and [advanced](https://en.wikibooks.org/wiki/LaTeX/Advanced_Mathematics) math formatting guides.

- **Collaborations:** The [Pro Git book](https://git-scm.com/book/en/v2), and a quick tutorial [here][Git and GitHub: A Quick Tutorial].

## Format

[**bookdown**](https://bookdown.org/yihui/bookdown/) is an R package which allows us to integrate scientific writing (including math, figures, etc.) with executable programming blocks.  While **bookdown** is interfaced through R, very little R knowledge is required to operate it.  The scientific writing is done in Markdown and LaTeX, whereas the programming blocks can be written in R, Python, Julia, C++, etc.

In terms of content, please focus on the following:

i.  Creating a clear organization of topics within your module using section/subsection/subsubsection headers.

ii.  Setting up the code, for example, in a clear and well-organized manner, i.e., what to include directly in the `.Rmd` file and what goes into external scripts, using lots of code comments, informative variable names, and consistent naming conventions, etc.

In terms of formatting the content with R Markdown, please refer to the [Formatting Guidelines](#appformattingguidelines).

<!-- The content of each module can roughly be divided into three components: -->

<!-- 1.  Text explaining the concepts, models, etc. -->

<!-- 2.  Short snippets of code which contribute to understanding the text (e.g., loading and displaying parts of data sets, simple calculations), or which can be used to create figures and tables. -->

<!-- 3.  Larger sections of code consisting of interdependent functions, data processing, complex plotting functions, etc.  Some of this code can be displayed within the text, but by and large, it should be written in -->

<!-- Each module should consist of:

- One `Rmd` file containing the Markdown/LaTeX text, and short snippets of code which contribute to understanding the text (e.g., loading and displaying parts of data sets, simple calculations), or which can be used to create figures and tables.

- Any number of external `.R`, `.py`, `.jl`, `.cpp`, etc. files for anything more complicated, e.g., interdependent and longer functions, data preprocessing files, etc.  I strongly recommend that you do not put too much code into `Rmd` files directly as it makes it much more difficult to organize, debug, and maintain.

- Do provide links to relevant resources: papers, blog posts, etc. which helped you understand things better, and also any *high quality/performance* software to use or explore.  I've tried to provide such resources in my notes; please do update if links are broken, libraries have been superceeded, etc. -->

<!-- the software.  I strongly recommend that you never try to write in multiple programming languages at once (e.g., R + Markdown). -->

<!-- A skeleton for the full set of notes (including all modules) is to be downloaded from GitHub.  Each module should consist of an `Rmd` file containing the Markdown/LaTeX text, and external `.R`, `.py`, `.jl`, `.cpp`, etc. files for the software.  I strongly recommend that you never try to write in multiple programming languages at once (e.g., R + Markdown). -->

### Typesetting Math

Please do take a look at the [LaTeX guides](#genguide-res:latex) above, as it is easy to do this right and very annoying to redo if you do it wrong.

One extremely useful LaTeX feature is the ability to define macros for commonly used commands.  I strongly encourage you to use macros whenever possible.  Not only does this save a considerable amount of typing, but also often an enormous amount of search-replacing.  For example, suppose I have a canonical transformation function which I'll be using throughout the document, but I'm not quite sure how to typeset it yet.  So I define the LaTeX macro `\ctran{}` and use it.  If later I want to redefine how `\ctran{}` is typeset that's an extremely easy fix.

Another useful LaTeX trick is to use simple patterns to define macros for common things.  For example, I always define bold letter names (which you should use for anything that isn't a scalar) as `\xx` `\YY`, etc., and bold symbol names as `\aal` for $\boldsymbol{\alpha}$, `\TTh` for $\boldsymbol{\Theta}$, etc. 

As a rule-of-thumb, **avoid** defining one-letter macros.  The worst LaTeX mistake I've ever made is to define $\beta$ as`\b` in everything LaTeX document I wrote before 2015.  Now suppose I want to replace $\beta$ with $\gamma$ as a symbol for the quantity of interest.  Then either I have an extremely confusing macro `\newcommand{\b}{\gamma}`, or the most annoying search-replace ever (`\begin{xyz}` anyone?).  Sometimes I do use one letter capital macros, e.g., `\newcommand{\N}{\mathcal{N}}` for the normal distribution, but these are much easier to search-replace.

For this project, LaTeX is rendered to HTML via [Mathjax](https://www.mathjax.org/), and thus the macros must be defined a bit differently than in regular LaTeX.  All LaTeX macros must be defined in [`preamble.html`](preamble.html).

### Citations {#genguide-citations}

For external websites, it's sufficient to provide a link as I've done in this document.  For journal articles, textbooks, conference proceedings, etc. it's good practice to provide the complete citation information (in addition to a link to the resource if it can be obtained legally for free).  For such references please use [BibTeX](https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management).

Add your citations to the bibliography file `references.bib`. To separate the citations for each module, add the name of your module using the comment sign $\%$ when you start to add citations. 

For citation labels, please use the standardized format with the author label followed by the last two digits of the year. The rules for the author labels are, if there is only one author, use `@firstauthorlastname`, if there are two authors, use `@firstauthorlastname.secondauthorlastname`, if there are more than 2 authors, use `@firstauthorlastname.etal`. Here's some examples [@gelman13], [@friedman.etal09]. 

## Rendering the Book

For this you will need to install the following packages:

```{r app-gen-pkgs, eval = FALSE}
install.packages(c("bookdown", "tidyverse", "wooldridge", "corrplot", "lmtest",
                   "caret", "PMCMRplus", "dunn.test", "geesmv", "emdbook", "coin", 
                   "randomForest", "boot", "e1071", "inTrees", "DescTools",
                   "pwr", "TOSTER", "foreach", "doParallel", "doSNOW"),
                 dependencies = TRUE)
```

Once this is done, quit + restart R for safe measure.  Then, from within the project folder, run

```{r app-gen-render, eval = FALSE}
bookdown::render_book()
```

To make sure everything is working as expected, you may wish to delete the `_book` and `_bookdown_files` folders before rendering.  These don't get cleaned up between runs, so there might be very old files hanging around in there if you don't periodially perform a manual cleanup.

<!--chapter:end:app_general_guidelines.Rmd-->

# Formatting Guidelines

*Author: Martin Lysy, Trang Bui*

*Last Updated: Feb 04, 2021*

--- 

## General {#formatguide-general}

- For a module, name your file starting with `mod_` and for a appendix, name your file starting with `app_`. 

- For headers, place a space between the last sharp symbol $\#$ and the name of the header.  

- Let's agree to use regular font for programming languages (e.g., R, Python, C++) and bold font for packages and libraries (e.g., **TMB**, **rstan**, **NumPy**, **Tensorflow**). Use `code` font for files, data sets, codes and arguments. For functions, use parentheses at the end, for example, `my_func()`. Use curly brackets `\bm{}` for LaTeX functions. Use `<kbd>Key</kbd>` for key strokes or buttons.

- Paste a link to packages which are mentioned for the first time. For R packages, you can use the `cran_link()` function to do this. For example, `r cran_link("rstan")`.

- Please see Appendix [B.2.3](#genguide-citations) for how to handle citations.

- To refer to figures/tables/sections, use the standard academic style is e.g., "Figure 2", "Section 2.5", etc.  In other words, the word is non-bold and non-italic, with the first letter capitalized, and the number following the word should have hyperlink. See relevant **bookdown** [chapter](https://bookdown.org/yihui/bookdown/figures.html) for how to do this. To refer to modules, use the text name, details can be found [here](https://stackoverflow.com/questions/57469501/cross-referencing-bookdownhtml-document2-not-working).

- Refer to `common_functions.R` for commonly used functions across modules before writing your own. 

- To make cross-referencing consistent in **bookdown**, for all labels -- code chunks, figures, tables, sections, equations, etc. -- use the prefix of the corresponding file as the prefix of any label. So please choose and use a module-specific prefix for each cross-reference. For example, use label `#formatguide` for this section. Refer to `prefixes.txt` for the list of the used prefixes and add yours there. Try to use the prefix that is distinctive to other words in the text such that you can find and replace all without mistakes.

- Give names to all code chunks, even e.g., pure code and figures you don't refer to.  **Note:** Chunk labels must consist of only letters, numbers, dashes (`-`), and slashes (`/`) -- no underscores!  Otherwise, cross-referencing [won't work properly](https://bookdown.org/yihui/bookdown/figures.html).

- Paragraph breaks are simply indicated by one or more empty lines.  No need for `\newline` or anything else.  This applies to elements other than text such as figures, math blocks etc.  If the figure/math block/etc is part of the paragraph, don't leave blank spaces before or after^[An exception to this is possibly within an indented region.  There you may need to leave a blank space before/after math block...I'm not 100% sure...].

- Please use Capital Case for all section headings, for example, Plan the Study.

- Our consensus for writing in a list is mentioned below. For details, visit [here](https://getitwriteonline.com/articles/vertical-lists/).
    - Use `-` to denote lists.
    - If the items in the list are complete sentences, begin the first word in each item with a capital-case letter and end the item with period.
    - If the items are single words and phrases, begin the first word in each item with a lower-case letter and end with a comma. Use "and" (or "or", depending on the context of the list) after the next-to-last item, and use a period after the last item in the list.
    - If the items are phrases or clauses with punctuations in them, put a semicolon at the end of each item. Put "and" (or "or") after the next-to-last item in the list and period for the last item. The items are not capitalized. 
    
- For indentation, use four spaces before your paragraph, code chunk, or equation. This will be very helpful when you have an item in an itemized list which contains more than one paragraph, equation, or code chunk.

- Name all code chunks according to the above guideline about cross-references. For code chunks that take long time to run, use `cache=TRUE` to prevent the chunk to be executed again unless the code in the chunk has been altered. More detail can be found [here](https://bookdown.org/yihui/rmarkdown-cookbook/cache.html).

- Use the package `r cran_link("styler")` to make the formatting of your code consistent. To do this, run the function `styler::style_file()` for your module's Rmarkdown file. Do not set the option `tidy = "styler"` to your chunk code to prevent caching issues. The details of the package can be found [here](https://styler.r-lib.org/).  

- Use "" instead of '' for quotes.

- Use underscore `_` instead of dot `.` to name variables.

## Embedding Files

To embed a file in your module, first put it in any subdirectory of the `data` folder of the repo.  For example, the repo currently contains the file `data/caliRain.csv`.  You can then use the file anywhere in your module, e.g.,

```{r app_embed}
head(read.csv("data/caliRain.csv"))
```

In order to let users download the file, you can simply provide the link using regular Markdown syntax.  In other words, `[download link](data/caliRain.csv)` will render as [download link](data/caliRain.csv).

**Note:** This method works because somewhere in the project we make a call to `knitr::include_graphics("data/arbitrary/file.ext")`, which has the effect of adding the `data` folder to the bookdown output directory.  In previous attempts we tried explicitly adding the `data` folder to the output via `dir.create()`. However, this requires that the output directory be supplied in advance, whereas in general it can be set on-the-fly via the `output_dir` argument to `bookdown::render_book()`.

<!-- - To embed a file in your module, use function `embed_data_file()`. For example, `r embed_data_file("data/caliRain.csv", text = "caliRain.csv")`. -->

## Math

- Always use bold font like $\XX$^[LaTeX purists might prefer to use `\mathbf{}` for roman letters instead of `\boldsymbol{}` as I have defined via the `\bm{}` macro in `preamble.html`.  Observe the difference: $\mathbf{x}$ vs $\boldsymbol{x}$ and $\mathbf{X}$ vs $\boldsymbol{X}$.  For pure LaTeX, the `\bm{}` command in the [**bm**](https://ctan.org/pkg/bm?lang=en) package automatically picks the right one for you.  Eventually, I'll figure out how to use this package for the e-book, in which case if you consistently use `\bm{}` for bold there will be very little for me to change!] or $\aal$ for anything that isn't a scalar.  In particular, please use `\bm{}` command in math mode to make things bold, and note that bold letters and symbols have standard macros as explained in `index.Rmd`.  Please use them!

- If applicable distinguish between vectors and matrices (or collections of vectors) with lower and upper case, like $\yy$ and $\YY$ or $\tth$ and $\TTh$.  

- If the math equations are part of sentences, end them with suitable punctuation. If the formula is a derivation with multiple steps, only put punctuation at the final step. 

- If an equation is to be written in a new line, use two dollar signs to start and finish the equation. For example, consider observations $\XX = (\rv X N)$ such that
    $$
    \begin{aligned} 
	  X_i \mid \mu_i & \ind \N(\mu_i, \sigma_i^2) \\
	  \mu_i & \iid \N(0, \tau^2).
	  \end{aligned}
	  (\#eq:formatguide-mathex)
    $$

- Generally, a colon should not be used to set off an equation, even when it is a displayed equation. Typically an equation is the object in a sentence. Use a colon on occasion for emphasis, but make sure it fits grammatically. Examples can be found [here](http://persweb.wabash.edu/facstaff/footer/courses/Expectations/WritingTips2.pdf). The same rules apply for code chunks.

- Use $j^{th}$ and $(i,j)^{th}$.

### Commonly-Used Math and Stats Expressions

- Sequences of random variables: $X_1, \ldots, X_N$.  In fact since these are so common, I've created a macro for it: $\rv Y 5$ and $\rv [m,0] Z {m,N_m}$.

- For multi-line equations with alignment, use `aligned` environment as in \@ref(eq:formatguide-mathex).

- The "d" in integrals:
    $$
    \ud X_t = \mu(X_t) \ud t + \ud B_t \qquad \iff \qquad X_t = \mu t + \int_0^t \mu(X_t) \ud t + B_t.
    $$

- Conditioning: $p(y \mid x)$.

- Independence: $Y \amalg X$.

- Probability: $\Pr(Y \le y)$.

- Derivatives and partials: These are quite time-consuming to typeset so I've created some macros for them:
    $$
	  \der{x} f(x), \qquad \fdel[3]{x} {g(x, y)}, \qquad \hess{\tth}\ell(\tth \mid \XX).
	  $$
	The last of these is used for Hessian matrices.

- Independent/IID/Normal: Please see \@ref(eq:formatguide-mathex).

- Variance/Covariance/Correlation: $\var(X), \cov(X), \cor(X)$.

- The $p$-value and the $F$, $t$ or chi-square tests.

<!--chapter:end:app_formatting_guidelines.Rmd-->

# Git and GitHub: A Quick Tutorial

*Author: Martin Lysy*

*Last Updated: Nov 09, 2020*

--- 

[Git](https://git-scm.com/) is an extremely powerful version control system that is used to "back up" software projects.  In a nutshell, rather than overwriting files every time you save them, Git saves the difference between the old and updated versions.  This gives you have a complete history of all saves with a minuscule memory overhead, making it very easy to roll back changes, see what you did when, etc.  It also has a branching system which is extremely useful to make patches or feature extensions for your software without affecting the "stable" version.

Git projects (or "repositories") can easily be stored on the cloud, which is ideal both for backups and for collaborative work.  The most common platform for this is [GitHub](https://github.com/).  You will need a GitHub account in order to access and submit files for this project.  It's completely free, secure, and does not collect or distribute your personal information.  You can delete your account at any time, but publicly visible projects can be made to look like a very nice webpage with minimal effort.  Therefore, in my opinion, this is a great way to showcase the software you develop for this class or others by providing a link to your GitHub page on your CV.

## Setup

- Install Git and learn some of the basics: https://githowto.com/.

- Create a free GitHub account: https://github.com/.

- Send me an email with your GitHub account information so I can add you to this project.

## Contributing to the Project

The following is the standard Git procedure for collaborating on a project^[If you have been given collaborator rights on a project, you don't actually need to fork the repo and can simply create branches on the main project from which to issue pull requests.  However, forking is the way to go for contributing bugfixes to public repos on your which you are not an official collaborator.  Also, it's very difficult for the project administrator (i.e., me) from preventing other collaborators from interfering with your branch on the main project, but by only you (and the people you authorize) can make changes to the branch on your fork.].

- Fork the project to your own GitHub account.  That is, you've now made a complete copy of the project that you can modify however you want without affecting the original.  This fork must be kept private (i.e., invisible to the outside world) for the duration of the course.

- While it is possible to edit the fork directly from GitHub, I strongly recommend against this as it's extremely inconvenient.  Instead, make a [local copy](https://help.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository) of the fork on your computer.

- In order to make changes to any Git repo, it is always recommended that you create a new [branch](https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell) for this first.  That is, assume that the stable version of the repo is on the `master` branch.  Then from the command line, you can do the following:

    ```bash
    # make sure you are on the master branch		
    git checkout master
    # create new branch
    git checkout -b mlysy-devel
    ```

	This will copy everything from `master` to a new branch called `mlysy-devel`.  Now I can make whatever changes I want to `mlysy-devel`.
	
- Commit your desired changes to the new branch.  When you are ready to send them back to the main project repo, follow these steps:

	1.  Push your local branch to your remote GitHub fork:
	
        ```bash
        git push -u origin mlysy-devel
        ```

	2.  Create a GitHub [pull request](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request-from-a-fork) via the button on your fork's page.

	When this is done, I will be able to inspect your changes and make suggestions before merging your work into the project-wide stable branch `master`.

## Git Mistakes

Git is an extremely versatile program with its own lingo (e.g. push/merge/stash/commit/stage/etc.), all of which takes some time to master.  The good news is that it's easy to "save everything" with Git and very hard to accidentally delete things.  To me since the point of using Git is version control, I usually don't worry about fixing little mistakes (like forgetting to include a file in a commit) that don't affect this overall goal.

There is however one type of Git mistake that's fairly annoying to fix.  This has to do with including large "object" files in the repo.  For the purpose of this discussion, object files are files which:

i.  Can be completely generated from source code in the repo, e.g., PDF files created with LaTeX, C++ shared object files (`.dll`, `.so`, `.o`), HTML files generated with R Markdown.

ii.  Cannot be read by a human with a plain text editor, e.g., PDF files, Microsoft Word documents, C++ shared object files, but not HTML files.

iii.  Get created over and over as the source code in the repo changes.

Object files should not be included in Git repos because they take up space and can easily be recreated from source.  C++ shared object files are especially useless since they are platform/system-dependent and thus probably won't work on your collaborator's computer.  Moreover, because object files are not human-readable, Git doesn't know how to compute the incremental difference between commits, and ends up saving the *entire object file* every time.  This can easily and needlessly bloat the size of the repo.

The nuisance here is that if you accidentally commit an object file, simply deleting it won't remove it from the Git history.  Pruning the Git history is a very annoying task, which is why I think of committing object files as the only real Git "mistake".  If you need to prune your Git history, please use this extremely handy [repo cleaner](https://rtyley.github.io/bfg-repo-cleaner/).

Rather than fixing object file commits after they happen, you can prevent this from happening by using [`.gitignore`](https://www.atlassian.com/git/tutorials/saving-changes/gitignore) file.  Simply put, `.gitignore` files tell Git to ignore certain files in the folder repo.  They can contain relative paths and [regular expressions](https://en.wikipedia.org/wiki/Regular_expression).  Each `.gitignore` file applies to the folder and subfolder of where it's located, so you can have "global" and "local" `.gitignore` files in the same repo.

The global `.gitignore` for this repo contains a number of common things to ignore in R and C++ projects.  Specific to this particular project, it also ignores the `_book` subfolder which contains the HTML-rendered e-book that you'll be building to preview on your local machine.

<!--chapter:end:app_git_tutorial.Rmd-->

<!-- ```{r setup, include = FALSE} -->
<!-- cran_link <- function(pkg) { -->
<!--   paste0("[**", pkg, "**](https://CRAN.R-project.org/package=", pkg, ")") -->
<!-- } -->
<!-- ``` -->

# Bookdown Tests

A few tests to make sure **bookdown** is rendering things correctly.

## Test Math Macros 

$\var(\xx)$, $\alpha_\xx$, $\xx \ind \N(0, \sigma_{\tx{pool}})$.

## Test Figure

```{r booktest-fig, fig.cap = "Test figure."}
plot(1:25, pch = 1:25)
```
The reference to Figure \@ref(fig:booktest-fig) is implemented correctly.  

## Test CRAN Link

The package link to `r cran_link("mniw")` works correctly.

## Test Checklist

Can users modify our checklists?  Apparently not.

- [ ] Item 1.

- [x] Item 2.

## Test References

The reference to @agresti15 works as expected.

<!--chapter:end:app_bookdown_tests.Rmd-->

`r if (knitr:::is_html_output()) '
# References {-}
'`

<!--chapter:end:Z_references.Rmd-->

